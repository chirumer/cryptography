{
  "source_pdf": "attacks.pdf",
  "relative_pdf_path": "papers/attacks.pdf",
  "num_pages": 18,
  "pages": [
    {
      "page_number": 1,
      "text": "Data Poisoning Attacks to Local Differential Privacy Protocols\nXiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong\nDuke University\n{xiaoyu.cao, jinyuan.jia, neil.gong}@duke.edu\nAbstract\nLocal Differential Privacy (LDP) protocols enable an un-\ntrusted data collector to perform privacy-preserving data an-\nalytics. In particular, each user locally perturbs its data to\npreserve privacy before sending it to the data collector, who\naggregates the perturbed data to obtain statistics of interest. In\nthe past several years, researchers from multiple communities–\nsuch as security, database, and theoretical computer science–\nhave proposed many LDP protocols. These studies mainly fo-\ncused on improving the utility of the LDP protocols. However,\nthe security of LDP protocols is largely unexplored.\nIn this work, we aim to bridge this gap. We focus on LDP\nprotocols for frequency estimation and heavy hitter identiﬁ-\ncation, which are two basic data analytics tasks. Speciﬁcally,\nwe show that an attacker can inject fake users into an LDP\nprotocol and the fake users send carefully crafted data to the\ndata collector such that the LDP protocol estimates high fre-\nquencies for arbitrary attacker-chosen items or identiﬁes them\nas heavy hitters. We call our attacksdata poisoning attacks.\nWe theoretically and/or empirically show the effectiveness of\nour attacks. We also explore three countermeasures against\nour attacks. Our experimental results show that they can effec-\ntively defend against our attacks in some scenarios but have\nlimited effectiveness in others, highlighting the needs for new\ndefenses against our attacks.\n1 Introduction\nVarious data breaches [1–3] have highlighted the challenges\nof relying on a data collector (e.g., Equifax) to protect users’\nprivate data. Local Differential Privacy (LDP), a variant of\ndifferential privacy [19], aims to address such challenges. In\nparticular, an LDP protocol encodes and perturbs a user’s data\nto protect privacy before sending it to the data collector, who\naggregates the users’ perturbed data to obtain statistics of\ninterest. Therefore, even if the data collector is compromised,\nuser privacy is still preserved as the attacker only has access\nto users’ privacy-preserving perturbed data. Because of the re-\nsilience against untrusted data collectors, LDP has attracted in-\ncreasing attention in both academia and industry. Speciﬁcally,\nmany LDP protocols [8\n–10,15,18,22,31–33,45,47,59–63,69]\nhave been developed in the past several years. Moreover, some\nof these protocols have been widely deployed in industry in-\ncluding but not limited to Google, Microsoft, and Apple. For\ninstance, Google deployed LDP [22] in the Chrome browser to\ncollect users’ default homepages for Chrome; Microsoft [17]\nintegrated LDP in Windows 10 to collect application usage\nstatistics; and Apple [53] adopted LDP on iOS to identify pop-\nular emojis, which are subsequently recommended to users.\nSince LDP perturbs each user’s data, it sacriﬁces utility of\nthe data analytics results obtained by the data collector. There-\nfore, existing studies on LDP mainly focused on improving\nthe utility via designing new methods to encode/perturb users’\ndata and aggregate the perturbed data to derive statistical\nresults. However, the security of LDP is largely unexplored.\nIn this work, we aim to bridge this gap. In particular, we\npropose a family of attacks called data poisoning attacksto\nLDP protocols. In our attacks, an attacker injects fake users\nto an LDP protocol and carefully crafts the data sent from the\nfake users to the data collector, with the goal to manipulate the\ndata analytics results as the attacker desires. Speciﬁcally, we\nfocus on LDP protocols for Frequency Estimationand Heavy\nHitter Identiﬁcation, which are two basic data analytics tasks\nand are usually the ﬁrst step towards more advanced tasks.\nThe goal of frequency estimation is to estimate the fraction\nof users (i.e., frequency) that have a certain item for each of\na set of items, while the goal of heavy hitter identiﬁcation\nis to only identify the top-k items that are the most frequent\namong the users without estimating the items’ frequencies.\nOur attacks can increase the estimated frequencies for arbi-\ntrary attacker-chosen items (called target items) in frequency\nestimation or promote them to be identiﬁed as top-k heavy hit-\nters in heavy hitter identiﬁcation. Our attacks result in severe\nsecurity threats to LDP-based data analytics. For example,\nan attacker can promote a phishing webpage as a popular\ndefault homepage of Chrome; an attacker can increase the\nestimated popularity of its (malicious) application when LDP"
    },
    {
      "page_number": 2,
      "text": "is used to estimate application popularity; and an attacker can\nmanipulate the identiﬁed and recommended popular emojis,\nresulting in bad user experience and frustration.\nThe major challenge of data poisoning attacks is that, given\na limited number of fake users an attacker can inject, what data\nthe fake users should send to the data collector such that the\nattack effectiveness is maximized. To address the challenge,\nwe formulate our attacks as an optimization problem, whose\nobjective function is to maximize the attack effectiveness and\nwhose solution is the data that fake users should send to the\ndata collector. We call our optimization-based attackMaximal\nGain Attack (MGA). To better demonstrate the effectiveness\nof MGA, we also propose two baseline attacks in which the\nfake users send randomly crafted data to the data collector.\nThen, we apply our MGA and the baseline attacks to three\nstate-of-the-art LDP protocols for frequency estimation (i.e.,\nkRR [33], OUE [59], and OLH [59]) and one state-of-the-art\nLDP protocol for heavy hitter identiﬁcation (i.e., PEM [62]).\nWe theoretically evaluate the effectiveness of our attacks.\nSpeciﬁcally, we derive the frequency gainof the target items,\nwhich is the difference of the target items’ estimated frequen-\ncies after and before an attack. Our theoretical analysis shows\nthat our MGA can achieve the largest frequency gain among\npossible attacks. Our theoretical results also show a funda-\nmental security-privacy tradeoff for LDP protocols: when an\nLDP protocol provides higher privacy guarantees, the LDP\nprotocol is less secure against our attacks (i.e., the frequency\ngains are larger). Moreover, we observe that different LDP\nprotocols have different security levels against our attacks. For\ninstance, OUE and OLH have similar security levels against\nour attacks, and kRR is less secure than OUE and OLH when\nthe number of items is larger than a threshold. We also empir-\nically evaluate our attacks for both frequency estimation and\nheavy hitter identiﬁcation using a synthetic dataset and two\nreal-world datasets. Our empirical results also show the effec-\ntiveness of our attacks. For example, on all the three datasets,\nour MGA can promote 10 randomly selected target items to\nbe identiﬁed as top-15 heavy hitters when the attacker only\ninjects 5% of fake users.\nWe also explore three countermeasures, i.e., normalization,\ndetecting fake users, and detecting the target item, to defend\nagainst our attacks. Speciﬁcally, in normalization, the data\ncollector normalizes the estimated item frequencies to be a\nprobability distribution, i.e., each estimated item frequency is\nnon-negative and the estimated frequencies of all items sum\nto 1. Since our attacks craft the data for the fake users via\nsolving an optimization problem, the data from the fake users\nmay follow certain patterns that deviate from genuine users.\nTherefore, in our second countermeasure, the data collector\naims to detect fake users via analyzing the statistical patterns\nof the data from the users, and the data collector ﬁlters the\ndetected fake users before estimating frequencies or identify-\ning heavy hitters. The third countermeasure detects the target\nitem without detecting the fake users when there is only one\ntarget item. Our empirical results show that these counter-\nmeasures can effectively defend against our attacks in some\nscenarios. For example, when the attacker has 10 target items,\nnormalization can reduce the frequency gain of our MGA to\nOUE from 1.58 to 0.46 and detecting fake users can reduce\nthe frequency gain to be almost 0 because the data collector\ncan detect almost all fake users. However, our attacks are still\neffective in other scenarios. For instance, when the attacker\nhas 10 randomly selected target items, our MGA to OLH still\nachieves a frequency gain of 0.43 even if both detecting fake\nusers and normalization are used. Our results highlight the\nneeds for new defenses against our attacks.\nIn summary, our contributions are as follows:\n• We perform the ﬁrst systematic study on data poisoning\nattacks to LDP protocols for frequency estimation and\nheavy hitter identiﬁcation.\n• We show that, both theoretically and/or empirically, our\nattacks can effectively increase the estimated frequencies\nof the target items or promote them to be identiﬁed as\nheavy hitters.\n• We explore three countermeasures to defend against our\nattacks. Our empirical results highlight the needs for new\ndefenses against our attacks.\n2 Background and Related Work\nWe consider LDP protocols for two basic tasks, i.e.,frequency\nestimation [10, 18, 22, 31–33, 59, 63, 64, 69] and heavy hit-\nter identiﬁcation [9, 45, 62]. Suppose there are n users. Each\nuser holds one item from a certain domain, e.g., the default\nhomepage of a browser. We denote the domain of the items\nas {1,2,··· ,d}. For conciseness, we simplify {1,2,··· ,d}as\n[d]. In frequency estimation, the data collector (also calledcen-\ntral server) aims to estimate the frequency of each item among\nthe n users, while heavy hitter identiﬁcation aims to identify\nthe top-k items that have the largest frequencies among the n\nusers. Frequency of an item is deﬁned as the fraction of users\nwho have the item.\n2.1 Frequency Estimation\nAn LDP protocol for frequency estimation consists of three\nkey steps: encode, perturb, and aggregate. The encode step\nencodes each user’s item into some numerical value. We\ndenote the space of encoded values as D. The perturb step\nrandomly perturbs the value in the space D and sends the per-\nturbed value to the central server. The central server estimates\nitem frequencies using the perturbed values from all users in\nthe aggregate step. For simplicity, we denote by PE(v) the\nperturbed encoded value for an item v. Roughly speaking, a\nprotocol satisﬁes LDP if any two items are perturbed to the\nsame value with close probabilities. Formally, we have the\nfollowing deﬁnition:"
    },
    {
      "page_number": 3,
      "text": "Deﬁnition 1(Local Differential Privacy). A protocolA sat-\nisﬁes ε-local differential privacy (ε-LDP) if for any pair of\nitems v1,v2 ∈ [d] and any perturbed valuey ∈ D, we have\nPr(PE(v1)= y) ≤eεPr(PE(v2)= y), where ε > 0 is called\nprivacy budget andPE(v) is the random perturbed encoded\nvalue of an item v.\nMoreover, an LDP protocol is called pure LDP if it satisﬁes\nthe following deﬁnition:\nDeﬁnition 2 (Pure LDP [59]). An LDP protocol is pure if\nthere are two probability parameters0 < q < p < 1 such that\nthe following equations hold for any pair of itemsv1,v2 ∈\n[d],v1 ̸= v2:\nPr(PE(v1) ∈{y|v1 ∈S(y)})= p (1)\nPr(PE(v2) ∈{y|v1 ∈S(y)})= q, (2)\nwhere S(y) is the set of items that y supports.\nWe note that the deﬁnition of the support S(y) depends on\nthe LDP protocol. For instance, for some LDP protocols [18,\n59], the support S(y) of a perturbed value y is the set of items\nwhose encoded values could be y. For a pure LDP protocol,\nthe aggregate step is as follows:\n˜fv =\n1\nn\nn\n∑\ni=1\n1S(yi )(v) −q\np −q , (3)\nwhere ˜fv is the estimated frequency for item v ∈[d], yi is the\nperturbed value from theith user, and1S(yi)(v) is an character-\nistic function, which outputs 1 if and only if yi supports item\nv. Formally, the characteristic function 1S(yi)(v) is deﬁned as\nfollows: 1S(y)(v) i s1i f v ∈S(y) and 0 otherwise.\nRoughly speaking, Equation (3) means that the frequency\nof an item is estimated as the fraction of users whose per-\nturbed values support the item normalized by p,q, and n.\nPure LDP protocols are unbiased estimators of the item fre-\nquencies [59], i.e., E[ ˜fv]= fv, where fv is the true frequency\nfor item v. Therefore, we have:\nn\n∑\ni=1\nE[1S(yi )(v)] =n( fv(p −q)+ q). (4)\nEquation (4) will be useful for the analysis of our attacks.\nNext, we describe three state-of-the-art pure LDP protocols,\ni.e., kRR [18], OUE [59], and OLH [59]. These three protocols\nare recommended for use in different scenarios. Speciﬁcally,\nkRR achieves the smallest estimation errors when the number\nof items is small, i.e., d < 3eε + 2. When the number of items\nis large, both OUE and OLH achieve the smallest estimation\nerrors. OUE has a larger communication cost, while OLH\nhas a larger computation cost for the central server. There-\nfore, when the communication cost is a bottleneck, OLH is\nrecommended, otherwise OUE is recommended.\n2.1.1 kRR\nEncode: kRR encodes an item v to itself. Therefore, the\nencoded space D for kRR is identical to the domain of items,\nwhich is D =[ d].\nPerturb: kRR keeps an encoded item unchanged with a\nprobability p and perturbs it to a different random item a ∈D\nwith probability q. Formally, we have:\nPr(y = a)=\n{\neε\nd−1+eε ≜ p, if a = v,\n1\nd−1+eε ≜ q, otherwise, (5)\nwhere y is the random perturbed value sent to the central\nserver when a user’s item is v.\nAggregate: The key for aggregation is to derive the support\nset. A perturbed value y only supports itself for kRR. Specif-\nically, we have S(y)= {y}. Given the support set, we can\nestimate item frequencies using Equation (3).\n2.1.2 OUE\nEncode: OUE encodes an item v to a d-bit binary vector eeev\nwhose bits are all zero except the v-th bit. The encoded space\nfor OUE is D = {0,1}d , where d is the number of items.\nPerturb: OUE perturbs the bits of the encoded binary vec-\ntor independently. Speciﬁcally, for each bit of the encoded\nbinary vector, if it is 1, then it remains 1 with a probability p.\nOtherwise if the bit is 0, it is ﬂipped to 1 with a probability q.\nFormally, we have:\nPr(yi = 1)=\n{\n1\n2 ≜ p, if i = v,\n1\neε+1 ≜ q, otherwise, (6)\nwhere the vector yyy =[ y1 y2 ··· yd ] is the perturbed value for\na user with item v.\nAggregate: A perturbed value yyy supports an item v if and\nonly if the v-th bit of yyy, denoted as yv, equals to 1. Formally,\nwe have S(yyy)= {v|v ∈[d] and yv = 1}.\n2.1.3 OLH\nEncode: OLH leverages a family of hash functionsH, each of\nwhich maps an item v ∈[d] to a value h ∈[d′], where d′< d.\nIn particular, OLH uses d′= eε + 1 as it achieves the best\nperformance [59]. An example of the hash function family\nH could be xxhash [ 14] with different seeds. Speciﬁcally,\na seed is a non-negative integer and each seed represents\na different xxhash hash function. In the encode step, OLH\nrandomly picks a hash function\nH from H. When xxhash\nis used, randomly picking a hash function is equivalent to\nrandomly selecting a non-negative integer as a seed. Then,\nOLH computes the hash value of the item v as h = H(v). The\ntuple (H,h) is the encoded value for the item v. The encoded\nspace for OLH is D = {(H,h)|H ∈H and h ∈[d′]}."
    },
    {
      "page_number": 4,
      "text": "Perturb: OLH only perturbs the hash value h and does not\nchange the hash function H. In particular, the hash value stays\nunchanged with probability p′ and switches to a different\nvalue in [d′] with probability q′. Formally, we have:\nPr(y =( H,a)) =\n{\neε\neε+d′−1 ≜ p′, if a = H(v),\n1\neε+d′−1 ≜ q′, otherwise, (7)\nwhere y is the perturbed value sent to the central server from a\nuser with item v. Therefore, the overall probability parameters\np and q are p = p′= eε\neε+d′−1 and q = 1\nd′·p′+(1− 1\nd′)·q′= 1\nd′.\nAggregate: A perturbed value y =( H,h) supports an item\nv ∈ [d] if v is hashed to h by H. Formally, we have S(y)=\n{v|v ∈[d] and H(v)= h}.\n2.2 Heavy Hitter Identiﬁcation\nThe goal of heavy hitter identiﬁcation [9, 10, 62] is to identify\nthe top-k items that are the most frequent among the n users.\nA direct and simple solution is to ﬁrst estimate the frequency\nof each item using a frequency estimation protocol and then\nselect the k items with the largest frequencies. However, such\nmethod is not scalable to a large number of items. In response,\na line of works [ 9, 10, 62] developed protocols to identify\nheavy hitters without estimating item frequencies. For ex-\nample, Bassily et al. [9] and Wang et al. [62] independently\ndeveloped a similar heavy hitter identiﬁcation protocol, which\ndivides users into groups and iteratively applies a frequency\nestimation protocol to identify frequent preﬁxes within each\ngroup. Next, we take thePreﬁx Extending Method (PEM)[62],\na state-of-the-art heavy hitter identiﬁcation protocol, as an\nexample to illustrate the process.\nIn PEM, each user encodes its item as a γ-bits binary vec-\ntor. Suppose users are evenly divided into g groups. In the\njth iteration, users in the jth group use the OLH protocol\nto perturb the ﬁrst λj = ⌈log2 k⌉+\n⌈\nj · γ−⌈log2 k⌉\ng\n⌉\nbits of their\nbinary vectors and send the perturbed bits to the central server,\nwhich uses the aggregate step of the OLH protocol to esti-\nmate the frequencies of the preﬁxes that extend the previous\ntop-k preﬁxes. OLH instead of OUE is used because the num-\nber of items corresponding to λj bits is 2λj , which is often\nlarge and incurs large communication costs for OUE. Specif-\nically, the central server uses the aggregate step of OLH to\nestimate the frequencies of the λj-bits preﬁxes in the set\nR j−1 ×{0,1}λj −λj−1 , where R j−1 is the set of top-k λj−1-bits\npreﬁxes identiﬁed in the ( j −1)th iteration and the ×symbol\ndenotes Cartesian product. After estimating the frequencies\nof these λj-bits preﬁxes, the central server identiﬁes the top-k\nmost frequent ones, which are denoted as the set R j. This pro-\ncess is repeated for the g groups and the set of top-k preﬁxes\nin the ﬁnal iteration are identiﬁed as the top-k heavy hitters.\n2.3 Data Poisoning Attacks\nData poisoning attacks to LDP protocols: A concurrent\nwork [13] studied untargeted attacks to LDP protocols. In\nparticular, they focused on degrading the overall performance\nof frequency estimation or heavy hitter identiﬁcation. For\ninstance, we can represent the estimated frequencies of all\nitems as a vector, where an entry corresponds to an item. They\nstudied how an attack can manipulate the Lp-norm distance\nbetween such vectors before and after attack. In contrast,\nwe study targeted attacks that aim to increase the estimated\nfrequencies of the attacker-chosen target items or promote\nthem to be identiﬁed as heavy hitters. We note that the Lp-\nnorm distance between the item frequency vectors is different\nfrom the increased estimated frequencies for the target items.\nFor instance, L1-norm distance between the item frequency\nvectors is a loose upper bound of the increased estimated\nfrequencies for the target items.\nData poisoning attacks to machine learning: A line of\nworks [7, 11, 23–25, 27–30, 35–39, 41–44, 49, 50, 58, 65] stud-\nied data poisoning attacks to machine learning systems. In\nparticular, the attacker manipulates the training data such\nthat a bad model is learnt, which makes predictions as the at-\ntacker desires. For instance, Biggio et al. [11] investigated data\npoisoning attacks against Support Vector Machines. Jagiel-\nski et al. [ 29] studied data poisoning attacks to regression\nmodels. Shafahi et al. [ 50] proposed poisoning attacks to\nneural networks, where the learnt model makes incorrect\npredictions only for target testing examples. Gu et al. [ 27]\nand Liu et al. [ 36] proposed data poisoning attacks (also\ncalled backdoor/trojan attacks) to neural networks, where\nthe learnt model predicts an attacker-chosen label for test-\ning examples with a certain trigger. Data poisoning attacks\nwere also proposed to spam ﬁlters [ 41], recommender sys-\ntems [24,25,35,65], graph-based methods [55], etc.. Our data\npoisoning attacks are different from these attacks because\nhow LDP protocols aggregate the users’ data to estimate fre-\nquencies or identify heavy hitters is substantially different\nfrom how a machine learning system aggregates training data\nto derive a model.\n3 Attacking Frequency Estimation\n3.1 Threat Model\nWe characterize our threat model with respect to an attacker’s\ncapability, background knowledge, and goal.\nAttacker’s capability and background knowledge:We as-\nsume an attacker can inject some fake users into an LDP\nprotocol. These fake users can send arbitrary data in the en-\ncoded space to the central server. Speciﬁcally, we assume\nn genuine users and the attacker injects m fake users to the\nsystem. Therefore, the total number of users becomes n + m.\nWe note that it is a practical threat model to assume that an"
    },
    {
      "page_number": 5,
      "text": "attacker can inject fake users.In particular, previous measure-\nment study [54] showed that attackers can easily have access\nto a large number of fake/compromised accounts in various\nweb services such as Twitter, Google, and Hotmail. Moreover,\nan attacker can buy fake/compromised accounts for these\nweb services from merchants in the underground market with\ncheap prices. For instance, a Hotmail account costs $0.004 –\n0.03; and a phone veriﬁed Google account costs $0.03 – 0.50\ndepending on the merchants.\nSince an LDP protocol executes the encode and perturb\nsteps locally on users’ side, the attacker has access to the\nimplementation of these steps. Therefore, the attacker knows\nvarious parameters of the LDP protocol. In particular, the\nattacker knows the domain size d, the encoded space D, and\nthe support set S(y) for each perturbed value y ∈D.\nAttacker’s goal: We consider the attacker’s goal is to pro-\nmote some target items, i.e., increase the estimated frequen-\ncies of the target items. For example, a company may be\ninterested in making its products more popular. Formally,\nwe denote by T = {t1,t2,··· ,tr} the set of r target items. To\nincrease the estimated frequencies of the target items, the at-\ntacker carefully crafts the perturbed values sent from the fake\nusers to the central server. We denote byY the set of crafted\nperturbed values for the fake users, where an entry yi of Y\nis the crafted perturbed value for a fake user. The perturbed\nvalue yi could be a number (e.g., for kRR protocol), a binary\nvector (e.g., for OUE), and a tuple (e.g., for OLH).\nSuppose ˜ft,b and ˜ft,a are the frequencies estimated by the\nLDP protocol for a target item t before and after attack, re-\nspectively. We deﬁne thefrequency gainΔ ˜ft for a target item\nt as Δ ˜ft = ˜ft,a − ˜ft,b,∀t ∈T . A larger frequency gain Δ ˜ft im-\nplies a more successful attack. Note that an LDP protocol\nperturbs the value on each genuine user randomly. Therefore,\nthe frequency gain Δ ˜ft is random for a given set of crafted\nperturbed values Y for the fake users. Thus, we deﬁne the\nattacker’s overall gain G using the sum of the expected fre-\nquency gains for the target items, i.e., G(Y)= ∑t∈T E[Δ ˜ft ],\nwhere Δ ˜ft implicitly depends on Y. Therefore, an attacker’s\ngoal is to craft the perturbed values Y to maximize the over-\nall gain. Formally, the attacker aims to solve the following\noptimization problem:\nmax\nY\nG(Y). (8)\nWe note that, to incorporate the different priorities of the\ntarget items, an attacker could also assign different weights to\nthe expected frequency gains E[Δ ˜ft ] of different target items\nwhen calculating the overall gain. Our attacks are also appli-\ncable to such scenarios. However, for simplicity, we assume\nthe target items have the same priority.\n3.2 Three Attacks\nWe propose three attacks: Random perturbed-value attack\n(RPA), random item attack (RIA), and Maximal gain attack\n(MGA). RPA selects a perturbed value from the encoded space\nof the LDP protocol uniformly at random for each fake user\nand sends it to the server. RPA does not consider any informa-\ntion about the target items. RIA selects a target item from the\nset of target items uniformly at random for each fake user and\nuses the LDP protocol to encode and perturb the item. MGA\ncrafts the perturbed value for each fake user to maximize the\noverall gain G via solving the optimization problem in Equa-\ntion (8). RPA and RIA are two baseline attacks, which are\ndesigned to better demonstrate the effectiveness of MGA.\nRandom perturbed-value attack (RPA): For each fake\nuser, RPA selects a value from the encoded space of the LDP\nprotocol uniformly at random and sends it to the server.\nRandom item attack (RIA):\nUnlike RPA, RIA considers in-\nformation about the target items. In particular, RIA randomly\nselects a target item from the set of target items for each fake\nuser. Then, the LDP protocol is applied to encode and perturb\nthe item. Finally, the perturbed value is sent to the server.\nMaximal gain attack (MGA):The idea behind this attack is\nto craft the perturbed values for the fake users via solving the\noptimization problem in Equation (8). Speciﬁcally, according\nto Equation (3), the frequency gain Δ ˜ft for a target item t is:\nΔ ˜ft =\n1\nn+m\nn+m\n∑\ni=1\n1S(yi )(t) −q\np −q −\n1\nn\nn\n∑\ni=1\n1S(yi )(t) −q\np −q (9)\n=\nn+m\n∑\ni=n+1\n1S(yi )(t)\n(n + m)(p −q) −\nm\nn\n∑\ni=1\n1S(yi )(t)\nn(n + m)(p −q) , (10)\nwhere yi is the perturbed value sent from user i to the server.\nThe ﬁrst term in Equation (10) only depends on fake users,\nwhile the second term only depends on genuine users. More-\nover, the expected frequency gain for a target itemt is:\nE[Δ ˜ft ]=\nn+m\n∑\ni=n+1\nE[1S(yi )(t)]\n(n + m)(p −q) −\nm\nn\n∑\ni=1\nE[1S(yi )(t)]\nn(n + m)(p −q) , (11)\nwhere we denote the second term as a constant ct for simplic-\nity. Moreover, based on Equation (4), we have:\nct = m( ft (p −q)+ q)\n(n + m)(p −q) , (12)\nwhere ft is the true frequency of t among the n genuine users.\nFurthermore, we have the overall gain as follows:\nG =\nn+m\n∑\ni=n+1\n∑\nt∈T\nE[1S(yi )(t)]\n(n + m)(p −q) −c, (13)\nwhere c = ∑t∈T ct = m( fT (p−q)+rq)\n(n+m)(p−q) , where fT = ∑t∈T ft . c\ndoes not depend on the perturbed values sent from the fake\nusers to the central server. In RPA and RIA, the crafted per-\nturbed values for the fake users are random. Therefore, the\nexpectation of the characteristic function E[1S(yi)(t)] and the\noverall gain depend on such randomness. However, MGA"
    },
    {
      "page_number": 6,
      "text": "uses the optimal perturbed values for fake users, and the char-\nacteristic function 1S(yi)(t) becomes deterministic. Therefore,\nfor MGA, we can drop the expectationE in Equation (13), and\nthen we can transform the optimization problem in Equation\n(8) as follows:\nY∗ = argmax\nY\nG(Y)= argmax\nY\nn+m\n∑\ni=n+1\n∑\nt∈T\n1S(yi )(t), (14)\nwhere we remove the constantsc and (n +m)(p −q) in the op-\ntimization problem. Note that the above optimization problem\nonly depends on the perturbed values of the fake users, and\nthe perturbed valuesyi for the fake users are independent from\neach other. Therefore, we can solve the optimization prob-\nlem independently for each fake user. Formally, for each fake\nuser, we craft its perturbed value y∗ via solving the following\noptimization problem:\ny∗ = argmax\ny∈D\n∑\nt∈T\n1S(y)(t). (15)\nWe note that, for each fake user, we obtain its perturbed\nvalue via solving the same above optimization problem. How-\never, as we will show in the next sections, the optimization\nproblem has many optimal solutions. Therefore, we randomly\npick an optimal solution for a fake user.\nNext, we discuss how to apply these three attacks to state-\nof-the-art LDP protocols including kRR, OUE, and OLH, as\nwell as analyzing their overall gains.\n3.3 Attacking kRR\nRandom perturbed-value attack (RPA):For each fake user,\nRPA randomly selects a perturbed value yi from the encoded\nspace, i.e., [d], and sends it to the server. We can calculate the\nexpectation of the characteristic function for t ∈T as follows:\nE[1S(yi )(t)] =Pr(1S(yi )(t)= 1) (16)\n= Pr(t ∈S(yi)) =Pr(yi = t) (17)\n= 1\nd (18)\nTherefore, according to Equation ( 13), the overall gain is\nG = rm\nd(n+m)(p−q) −c.\nRandom item attack (RIA): For each fake user, RIA ran-\ndomly selects an item ti from the set of target items T , per-\nturbs the item following the rule in Equation (5), and sends\nthe perturbed item yi to the server. First, we can calculate the\nexpectation of the characteristic function as follows:\nE[1S(yi )(t)] =Pr(yi = t) (19)\n= Pr(ti = t)Pr(yi = t|ti = t)\n+ Pr(ti ̸= t)Pr(yi = t|ti ̸= t) (20)\n= 1\nr ·p +( 1 − 1\nr )q, (21)\nwhere r = |T | is the number of target items. According\nto Equation ( 13), we can obtain the overall gain as G =\n(p+(r−1)q)m\n(n+m)(p−q) −c.\nMaximal gain attack (MGA): For each fake user, MGA\ncrafts its perturbed value by solving the optimization prob-\nlem in Equation ( 15). For the kRR protocol, we have\n∑t∈T 1S(y)(t) ≤ 1 and ∑t∈T 1S(y)(t)= 1 when y is a target\nitem in T . Therefore, MGA picks any target item for each\nfake user. Moreover, according to Equation (13), the overall\ngain is G = m\n(n+m)(p−q) −c.\n3.4 Attacking OUE\nRandom perturbed-value attack (RPA):For each fake user,\nRPA selects a d-bits binary vector yyyi from the encoded space\n{0,1}d uniformly at random as its perturbed vector and sends\nit to the server. We denote by yi, j the j-th bit of the per-\nturbed vector yyyi. Therefore, for each target item t ∈ T ,w e\nhave E[1S(yyyi)(t)] =Pr(yi,t = 1)= 1\n2 . According to Equation\n(13), we can obtain the overall gain as G = rm\n2(n+m)(p−q) −c.\nRandom item attack (RIA): For each fake user, RIA ran-\ndomly selects a target itemti ∈T , encodes it to ad-bits binary\nvector ei whose bits are all zeros except theti-th bit, randomly\nperturbs ei following Equation (6), and sends the perturbed\nvector yyyi to the server. For a target itemt ∈T , we can calculate\nthe expected value of the characteristic function as follows:\nE[1S(yyyi )(t)] =Pr(yi,t = 1) (22)\n= Pr(ti = t)Pr(yi,t = 1|ti = t)\n+ Pr(ti ̸= t)Pr(yi,t = 1|ti ̸= t) (23)\n= 1\nr ·p +( 1 − 1\nr ) ·q, (24)\nwhere p and q are deﬁned in Equation ( 6). Therefore, the\noverall gain is G = (p+(r−1)q)m\n(n+m)(p−q) −c.\nMaximal gain attack (MGA): For each fake user, MGA\nchooses a perturbed vectoryyyi that is a solution of the optimiza-\ntion problem deﬁned in Equation ( 15). For OUE, we have\n∑t∈T 1S(yyyi)(t) ≤ r and ∑t∈T 1S(yyyi)(t)= r is achieved when\n1S(yyyi)(t)= 1,∀t ∈T . Thus, for each fake user, MGA initial-\nizes a perturbed vector yyyi as a binary vector of all 0’s and sets\nyi,t = 1 for all t ∈T . However, if all fake users send the same\nperturbed binary vector to the server, the server can easily\ndetect the fake users. For instance, there is only one entry in\nthe perturbed binary vector that has value 1 when we only\nhave 1 target item; and the server could detect a vector with\nonly a single 1 to be from a fake user, because it is statistically\nunlikely for a genuine user to send such a vector. Therefore,\nMGA also randomly samplesl non-target bits of the perturbed\nvector yyyi and sets them to 1. Speciﬁcally, we set l such that\nthe number of 1’s in the binary vector is the expected number\nof 1’s in the perturbed binary vector of a genuine user. Since\nthe perturbed binary vector of a genuine user hasp +(d −1)q\n1’s on average, we set l = ⌊p +( d −1)q −r⌋. Note that r is\nusually much smaller than d,s o l is a non-negative value. The\nﬁnal binary vector is sent to the server. According to Equation\n(13), the overall gain is G = rm\n(n+m)(p−q) −c."
    },
    {
      "page_number": 7,
      "text": "kRR OUE OLH\nRandom perturbed-value attack (RPA) β( r\nd − fT ) β(r − fT ) −β fT\nRandom item attack (RIA) β(1 − fT ) β(1 − fT ) β(1 − fT )\nMaximal gain attack (MGA) β(1 − fT )+ β(d−r)\neε−1 β(2r − fT )+ 2βr\neε−1 β(2r − fT )+ 2βr\neε−1\nStandard deviation of estimation r\n√\nd−2+eε\n(eε−1)√n\n2reε/2\n(eε−1)√n\n2reε/2\n(eε−1)√n\nTable 1: Overall gains of the three attacks for kRR, OUE, and OLH.n is the number of genuine users,β = m\nn+m is the\nfraction of fake users among all users,d is the number of items,r is the number of target items,fT = ∑t∈T ft is the sum\nof true frequencies of the target items among the genuine users,ε is the privacy budget, ande is the base of the natural\nlogarithm. To understand the signiﬁcance of the overall gains, we also include the standard deviations of the estimated\ntotal frequencies of the target items among then genuine users [59] in the table.\n3.5 Attacking OLH\nRandom perturbed-value attack (RPA):For each fake user,\nRPA randomly selects a hash functionHi ∈H and a hash value\nai ∈[d′], and sends the tuple yi =( Hi,ai) to the server. For\neach t ∈T ,w eh a v eE[1S(yyyi)(t)] =Pr(Hi(t)= ai)= 1\nd′. There-\nfore, we can obtain the overall gain as G = rm\nd′(n+m)(p−q) −c.\nRandom item attack (RIA): For each fake user, RIA ran-\ndomly selects a target itemti, randomly selects a hash function\nHi ∈H, and calculates the hash value hi = Hi(ti). The tuple\n(Hi,hi) is then perturbed as (Hi,ai) according to Equation (7).\n(Hi,ai) is the perturbed value, i.e., yi =( Hi,ai). We assume\nthe hash function Hi maps any item in [d] to a value in [d′]\nuniformly at random. For a target itemt ∈T , we can calculate\nthe expectation of the characteristic function as follows:\nE[1S(yi )(t)] =Pr(Hi(t)= ai) (25)\n= Pr(ti = t)Pr(Hi(t)= ai|ti = t)\n+ Pr(ti ̸= t)Pr(Hi(t)= ai|ti ̸= t) (26)\n= 1\nr ·p +( 1 − 1\nr ) ·q. (27)\nThus, the overall gain for RIA is G = [p+(r−1)q]m\n(n+m)(p−q) −c.\nMaximal gain attack (MGA): For each fake user, MGA\nchooses a perturbed value yi =( Hi,ai) that is a solution of\nthe optimization problem deﬁned in Equation (15). For OLH,\nwe have ∑t∈T 1S(yi)(t) ≤r and ∑t∈T 1S(yi)(t)= r is achieved\nwhen the hash function Hi maps all items in T to ai, i.e.,\nHi(t)= ai,∀t ∈T . Thus, for each fake user, MGA searches\nfor a hash function Hi in H such that Hi(t)= ai,∀t ∈T holds.\nTherefore, according to Equation ( 13), the overall gain is\nG = rm\n(n+m)(p−q) −c. Note that we may not be able to ﬁnd such\na hash function in practice. In our experiments, for each fake\nuser, we randomly sample 1,000 hash functions and use the\none that hashes the most target items to the same value.\n3.6 Theoretical Analysis\nTable 1 summarizes the overall gains of the three attacks for\nkRR, OUE, and OLH, where we have replaced the parameters\np and q for each LDP protocol according to Section2.1. Next,\nwe compare the three attacks, discuss a fundamental security-\nprivacy tradeoff, and compare the three LDP protocols with\nrespect to their security against our data poisoning attacks.\nComparing the three attacks: All three attacks achieve\nlarger overall gains when the target items’ true frequencies\nare smaller (i.e.,\nfT is smaller). MGA achieves the largest\noverall gain among the three attacks. In fact, given an LDP\nprotocol, a set of target items and fake users, MGA achieves\nthe largest overall gain among all possible attacks. This is\nbecause MGA crafts the perturbed values for the fake users\nsuch that the overall gain is maximized. RIA achieves larger\noverall gains than RPA for kRR and OLH, while RPA achieves\na larger overall gain than RIA for OUE.\nTable 1 also includes the standard deviations of the es-\ntimated total frequencies of the target items among the n\ngenuine users. Due to the √n term in the denominators, the\nstandard deviations are much smaller than the overall gains\nof our MGA attacks. For instance, on the Zipf dataset in our\nexperiments with the default parameter settings, the overall\ngains of MGA are 1600, 82, and 82 times larger than the\nstandard deviations for kRR, OUE, and OLH, respectively.\nFundamental security-privacy tradeoffs: The security of\nan LDP protocol is determined by the strongest attack (i.e.,\nMGA) to it. Intuitively, when the privacy budgetε is smaller\n(i.e., stronger privacy), genuine users add larger noise to their\ndata. However, the perturbed values that MGA crafts for the\nfake users do not depend on the privacy budget. As a result,\nthe fake users contribute more towards the estimated item\nfrequencies, making the overall gain larger. In other words,\nwe have a fundamental security-privacy tradeoff. Formally,\nthe following theorem shows such tradeoffs.\nTheorem 1(Security-Privacy Tradeoff). F or any of the three\nLDP protocols kRR, OUE, and OLH, when the privacy budget\nε is smaller (i.e., stronger privacy), MGA achieves a larger\noverall gain G (i.e., weaker security).\nProof. Table 1 shows that ε is in the denominator of the\noverall gains for MGA. Therefore, the overall gains of MGA\nincrease as ε decreases."
    },
    {
      "page_number": 8,
      "text": "Comparing the security of the three LDP protocols:Ta-\nble 1 shows that, when MGA is used, OUE and OLH achieve\nthe same overall gain. Therefore, OUE and OLH have the\nsame level of security against data poisoning attacks. The\nfollowing theorem shows that OUE and OLH are more secure\nthan kRR when the number of items is larger than a threshold.\nTheorem 2. Suppose MGA is used. OUE and OLH are more\nsecure than kRR when the number of items is larger than some\nthreshold, i.e., d> (2r −1)(eε −1)+ 3r.\nProof. See Appendix A.\n4 Attacking Heavy Hitter Identiﬁcation\n4.1 Threat model\nAttacker’s capability and background knowledge: We\nmake the same assumption on the attacker’s capability and\nbackground knowledge as in attacking frequency estimation,\ni.e., the attacker can inject fake users into the protocol and\nsend arbitrary data to the central server.\nAttacker’s goal: We consider the attacker’s goal is to pro-\nmote some target items, i.e., manipulate the heavy hitter iden-\ntiﬁcation protocol to recognize the target items as top-k heavy\nhitters. Formally, we denote byT = {t1,t2,··· ,tr}the set of r\ntarget items, which are not among the true top-k heavy hitters.\nWe deﬁne success rate of an attack as the fraction of target\nitems that are promoted to be top-k heavy hitters by the attack.\nAn attacker’s goal is to achieve a high success rate.\n4.2 Attacks\nState-of-the-art heavy hitter identiﬁcation protocols iteratively\napply frequency estimation protocols. Therefore, we apply\nthe three attacks for frequency estimation to heavy hitter iden-\ntiﬁcation. Next, we use PEM as an example to illustrate how\nto attack heavy hitter identiﬁcation protocols.\nIn PEM, each item is encoded by a γ-bits binary vector\nand users are randomly divided into g groups. On average,\neach group contains a fraction of m\nn+m fake users. In the jth\niteration, PEM uses OLH to perturb the ﬁrst λj bits of the\nbinary vectors for users in the jth group and sends them to\nthe central server. An attacker uses the RPA, RIA, or MGA to\ncraft the data sent from the fake users to the central server by\ntreating the ﬁrst λj bits of the binary vectors corresponding\nto the target items as the “target items” in the jth iteration.\nSuch attacks can increase the likelihood that the ﬁrst λj bits\nof the target items are identiﬁed as the top-k preﬁxes in the\njth iteration, which in turn makes it more likely to promote\nthe target items as top-k heavy hitters.\nParameter Default setting\nβ 0.05\nr 1\nfT 0.01\nε 1\nk 20\ng 10\nTable 2: Default parameter settings.\n5 Evaluation\n5.1 Experimental Setup\nDatasets: We evaluate our attacks on three datasets, in-\ncluding a synthetic dataset and two real-world datasets, i.e.,\nFire [4] and IPUMS [51].\n• Zipf: Following previous work on LDP protocols, we\ngenerate random data following the Zipf’s distribution. In\nparticular, we use the same parameter in the Zipf’s distri-\nbution as in [59]. By default, we synthesize a dataset with\n1,024 items and 1,000,000 users.\n• Fire [4]: The Fire dataset was collected by the San Fran-\ncisco Fire Department, recording information about calls\nfor service. We ﬁlter the records by call type and use the\ndata of type “Alarms”. We treat the unit ID as the item that\neach user holds, which results in a total of 244 items and\n548,868 users.\n• IPUMS [51]: The IPUMS dataset contains the US census\ndata over the years. We select the latest data of 2017 and\ntreat the city attribute as the item each user holds, which\nresults in a total of 102 items and 389,894 users.\nParameter setting: For frequency estimation, the overall\ngains of our attacks may depend on β (the fraction of fake\nusers), r and fT (the number of target items and their true\nfrequencies), ε (privacy budget), and d (number of items in\nthe domain). For heavy hitter identiﬁcation, the success rates\nof our attacks further depend on k (the number of items iden-\ntiﬁed as heavy hitters) and g (the group size used by the\nPEM protocol). Table 2 shows the default settings for these\nparameters, which we will use in our experiments unless other-\nwise mentioned. We will study the impact of each parameter,\nwhile ﬁxing the remaining parameters to their default settings.\nMoreover, we use d′= ⌈eε + 1⌉in OLH as d′is an integer.\n5.2 Results for Frequency Estimation\nImpact of different parameters:Table 1 shows the theoret-\nical overall gains of the three attacks for the kRR, OUE, and\nOLH protocols. We use these theoretical results to study the\nimpact of each parameter. Figures 1 to 3 show the impact of\ndifferent parameters on the overall gains andnormalized over-\nall gains. A normalized overall gain is the ratio between the\ntotal frequencies of the target items after and before an attack,"
    },
    {
      "page_number": 9,
      "text": "Figure 1: Impact of different parameters on the overall gains (ﬁrst row) and normalized overall gains (second row) of\nthe three attacks for kRR.\nFigure 2: Impact of different parameters on the overall gains (ﬁrst row) and normalized overall gains (second row) of\nthe three attacks for OUE.\nFigure 3: Impact of different parameters on the overall gains (ﬁrst row) and normalized overall gains (second row) of\nthe three attacks for OLH.\ni.e., (G + fT )/ fT , where fT is the total true frequencies of the\ntarget items. We observe that MGA outperforms RIA, which\noutperforms RPA or achieves similar (normalized) overall\ngains with RPA. The reason is that MGA is an optimization-\nbased attack, RIA considers information of the target items,\nand RPA does not consider information about the target items.\nNext, we focus our analysis on MGA since it is the strongest\nattack. The (normalized) overall gains of MGA increase as\nthe attacker injects more fake users, the attacker promotes\nmore target items (except the kRR protocol), or the privacy\nbudget\nε becomes smaller (i.e., security-privacy tradeoffs).\nThe (normalized) overall gain of MGA decreases as the total\ntrue frequency of the target items (i.e., fT ) increases, though\nthe decrease of the overall gain is marginal. The (normalized)\noverall gain of MGA increases for kRR but keeps unchanged\nfor OUE and OLH as d increases. We note that, for a given"
    },
    {
      "page_number": 10,
      "text": "Figure 4: Impact of different parameters on the success rates of the three attacks for PEM (heavy hitter identiﬁcation\nprotocol). The ﬁrst row is on Zipf, the second row is on Fire, and the third row is on IPUMS.\nset of target items (i.e., fT is given), the trend of normalized\noverall gain is the same as that of the overall gain with respect\nto parameters\nβ, r, ε, and d. Therefore, in the rest of the paper,\nwe focus on overall gain for simplicity.\nMeasuring RIA and MGA for OLH:The theoretical over-\nall gain of RIA for OLH is derived based on the “perfect”\nhashing assumption, i.e., an item is hashed to a value in the\nhash domain [d′] uniformly at random. Practical hash func-\ntions may not satisfy this assumption. Therefore, the theoreti-\ncal overall gain of RIA for OLH may be inaccurate in practice.\nWe use xxhash [14] as hash functions to evaluate the gaps be-\ntween the theoretical and practical overall gains. In particular,\nFigure 5a compares the theoretical and practical overall gains\nof RIA for OLH, where 1 item is randomly selected as target\nitem, β = 0.05, and ε = 1. We observe that the theoretical and\npractical overall gains of RIA for OLH are similar.\nOur theoretical overall gain of MGA for OLH is derived\nbased on the assumption that the attacker can ﬁnd a hash\nfunction that hashes all target items to the same value. In\npractice, we may not be able to ﬁnd such hash functions\nwithin a given amount of time. Therefore, for each fake user,\nwe randomly sample some xxhash hash functions and use\nthe one that hashes the most target items to the same value.\nFigure 5b compares the theoretical and practical overall gains\nof MGA for OLH on the IPUMS dataset as we sample more\nhash functions for each fake user, where we randomly select\n5 items as target items, i.e., r = 5. Our results show that the\npractical overall gains approach the theoretical ones with\nseveral hundreds of randomly sampled hash functions when\nr = 5. We have similar observations for the other two datasets\nand thus we omit their results due to the limited space.\n(a) (b)\nFigure 5: (a) Theoretical and practical overall gains of\nRIA for OLH. (b) Theoretical and practical overall gains\nof MGA for OLH on the IPUMS dataset as we sample\nmore hash functions for each fake user, wherer = 5.\n5.3 Results for Heavy Hitter Identiﬁcation\nFigure 4 shows the empirical results of applying our three\nattacks, i.e., RPA, RIA and MGA, to PEM on the Zipf, Fire,\nand IPUMS datasets, respectively. By default, we randomly\nselect r = 10 target items that are not identiﬁed as top-k heavy\nhitters by PEM before attack and use the three attacks to\npromote them. Default values for the other parameters are\nidentical to those in Table 2. The success rate of an attack\nis calculated as the fraction of target items that appear in\nthe estimated top-\nk heavy hitters. The results show that our\nMGA attacks can effectively compromise the PEM protocol.\nIn particular, we observe that MGA only needs about 5% of\nfake users to achieve a 100% success rate when r = 10 and\nk = 20. In fact, with only 5% of fake users, we can promote\n10 target items to be in the top-15 heavy hitters, or promote 15\ntarget items to be in the top-20 heavy hitters. However, RPA\nand RIA are ineffective. Speciﬁcally, even if we inject 10%"
    },
    {
      "page_number": 11,
      "text": "of fake users, neither RPA nor RIA can successfully promote\neven one of the target items to be in the top-k heavy hitters.\nMoreover, the number of groups g and the privacy budget ε\nhave negligible impact on the effectiveness of our attacks.\n6 Countermeasures\nWe explore three countermeasures. The ﬁrst countermeasure\nis to normalize the estimated item frequencies to be a prob-\nability distribution, the second countermeasure is to detect\nfake users via frequent itemset miningof the users’ perturbed\nvalues and remove the detected fake users before estimating\nitem frequencies, and the third countermeasure is to detect the\ntarget item without detecting the fake users when there is only\none target item. The three countermeasures are effective in\nsome scenarios. However, our MGA is still effective in other\nscenarios, highlighting the needs for new defenses against our\ndata poisoning attacks.\n6.1 Normalization\nThe LDP protocols estimate item frequencies using Equation\n(3). Therefore, the estimated item frequencies may not form a\nprobability distribution, i.e., some estimated item frequencies\nmay be negative and they may not sum to 1. For instance, our\nexperimental results in Section 5.2 show that the overall gains\nof MGA may be even larger than 1. Therefore, one natural\ncountermeasure is to normalize the estimated item frequencies\nsuch that each estimated item frequency is non-negative and\nthe estimated item frequencies sum to 1. For instance, one\nnormalization we consider is as follows: the central server\nﬁrst estimates the frequency\n˜fv for each item v following a\nLDP protocol (kRR, OUE, or OLH); then the server ﬁnds\nthe minimal estimated item frequency ˜fmin; ﬁnally, the server\ncalibrates the estimated frequency for each item v as ¯fv =\n˜fv− ˜fmin\n∑v( ˜fv− ˜fmin) , where ¯fv is the calibrated frequency. Our overall\ngain is calculated by the difference between the calibrated\nfrequencies of the target items after and before attack. We note\nthat there are also other methods to normalize the estimated\nitem frequencies [ 31, 63], which we leave as future work.\nNote that the normalization countermeasure is not applicable\nto heavy hitter identiﬁcation because normalization does not\nimpact the ranking of items’ frequencies.\n6.2 Detecting Fake Users\nRPA and MGA directly craft the perturbed values for fake\nusers, instead of using the LDP protocol to generate the per-\nturbed values from certain items. Therefore, the perturbed\nvalues for the fake users may be statistically abnormal. We\nnote that it is challenging to detect fake users via statistical\nanalysis of the perturbed values for the kRR protocol, because\nthe perturbed value of a user is just an item, no matter whether\nUser 1:\nUser 2:\nUser 3:\nUser 4:\nFigure 6: An example itemset that are all 1’s in 3 of the 4\nbinary vectors. Each column corresponds to an item.\nor not the attacker follows the protocol to generate the per-\nturbed value. Therefore, we study detecting fake users in the\nRPA and MGA attacks for the OUE and OLH protocols. Since\nPEM iteratively applies OLH, we can also apply detecting\nfake users to PEM.\nOUE: Recall that MGA assigns 1 to all target items and l\nrandomly selected items in the perturbed binary vector for\neach fake user. Therefore, among the perturbed binary vectors\nfrom the fake users, a set of items will always be 1. However,\nif the perturbed binary vectors follow the OUE protocol, it\nis unlikely to observe that this set of items are all 1’s for a\nlarge number of users. Therefore, our idea to detect fake users\nconsists of two steps. In the ﬁrst step, the server identiﬁes\nitemsets that are all 1’s in the perturbed binary vectors of a\nlarge number of users. In the second step, the server detects\nfake users if the probability that such large number of users\nhave these itemsets of all 1’s is small, when following OUE.\nStep I. In this step, the server identiﬁes itemsets that are\nfrequently all 1’s among the perturbed binary vectors. Figure6\nshows an example itemset that are all 1’s in 3 of the 4 binary\nvectors. Identifying such itemsets is also known as frequent\nitemset mining[6]. In our problem, given the perturbed binary\nvectors from all users, frequent itemset mining can ﬁnd the\nitemsets that are all 1’s in at least a certain number of users.\nSpeciﬁcally, a frequent itemset mining method produces some\ntuples BBB = {(B,s)|s ≥τ}, where B is an itemset and s is the\nnumber of users whose perturbed binary vectors are 1’s for\nall items in B.\nStep II. In this step, we determine whether there are fre-\nquent itemsets that are statistically abnormal. Speciﬁcally,\nwe predict a tuple (B,s) ∈BBB to be abnormal if s ≥τz, where\nz = |B| is the size of the itemset B. When an itemset is pre-\ndicted to be abnormal, we predict the items as the target items\nand the users whose perturbed binary vectors are 1’s for all\nitems in the itemset to be fake. The threshold τz achieves a\ntradeoff between false positive rateand false negative rateof\ndetecting fake users. Speciﬁcally, when τz is larger, a smaller\nnumber of genuine users are predicted as fake (i.e., a smaller\nfalse positive rate), while a larger number of fake users are\nnot detected (i.e., a larger false negative rate). Therefore, a\nkey challenge is how to select the threshold τz. We propose\nto select the threshold such that the false positive rate is at\nmost\nη. Speciﬁcally, given a threshold τz > (n + m)pqz−1,"
    },
    {
      "page_number": 12,
      "text": "we can derive an upper bound of the false positive rate as\n(n+m)pqz−1(1−pqz−1)\n[τz−(n+m)pqz−1]2 (see Appendix B for details). Therefore,\nto guarantee that the false positive rate is at mostη and achieve\na small false negative rate, we select the smallest τz that sat-\nisﬁes τz > (n + m)pqz−1 and (n+m)pqz−1(1−pqz−1)\n[τz−(n+m)pqz−1]2 ≤η.W es e t\nη = 0.01 in our experiments.\nOLH: To attack the OLH protocol, MGA searches a hash\nfunction for each fake user that hashes as many target items\nto the same value as possible. Suppose we construct a d-\nbit binary vector yyy for each user with a tuple (H,a) such\nthat yv = 1 if and only if H(v)= a. Then, the target items\nwill be 1’s in the binary vectors for a large number of users.\nTherefore, we can also leverage the method to detect fake\nusers in OLH. Speciﬁcally, in Step I, we ﬁnd frequent item-\nsets in the constructed binary vectors. In Step II, we predict\nan itemset B to be abnormal if its number of occurrences s\namong the n + m binary vectors is larger than a threshold\nτz, where z = |B| is the size of the itemset. Like OUE, we\nselect the threshold τz such that the false positive rate is at\nmost η. Speciﬁcally, we select the smallest τz that satisﬁes\nI(qz−1;τz,n + m −τz + 1) ≤η, where I is the regularized in-\ncomplete beta function[5]. I(qz−1;τz,n + m −τz + 1) is the\nfalse positive rate for a given τz (see Appendix B for details).\nPEM: The heavy hitter identiﬁcation protocol PEM itera-\ntively applies OLH to identify heavy hitters. Therefore, we\ncan apply the frequent itemset mining based detection method\nto detect fake users in PEM. Speciﬁcally, in each iteration of\nPEM, the central server applies the detection method in OLH\nto detect fake users in PEM; and the central server removes\nthe predicted fake users before computing the top-k preﬁxes.\n6.3 Conditional Probability based Detection\nThe frequent itemset mining based detection method above\nrequires at least two target items as it identiﬁes the abnor-\nmal frequent itemset as the target items. When there is only\none target item, i.e., r = 1, it fails to detect the target item.\nTherefore, we discuss another method to detect the target item\nwhen r = 1, which leverages conditional probabilities. Note\nthat this method does not detect fake users.\nOUE: Suppose yyy is a user’s perturbed binary vector. With a\nlittle abuse of notation, we denote the j-th bit of yyy as y j.G i v e n\nthe target item t and a random item j, we have the following\nequations under our MGA attacks to OUE:\nPr(y j = yt = 1)= Pr(v = t) ·Pr(y j = yt = 1|v = t)\n+ Pr(v = j) ·Pr(y j = yt = 1|v = j)\n+ Pr(v ̸= t, j) ·Pr(y j = yt = 1|v ̸= t, j)\n+ Pr(fake) ·Pr(y j = yt = 1|fake) (28)\n= nf t\nn + m ·pq + nf j\nn + m ·pq\n+ n(1 − ft − f j )\nn + m ·q2 + m\nn + m · l\nd −1 , (29)\nf j 0.01 0.01 0.1 0.1 0.5 0.5 0.9 0.9\nft 0 0.01 0 0.01 0 0.01 0 0.01\nˆfu 0.25 0.26 0.18 0.19 0.18 0.18 0.18 0.19\n(a) β = 0.05\nf j 0.01 0.01 0.1 0.1 0.5 0.5 0.9 0.9\nft 0 0.01 0 0.01 0 0.01 0 0.01\nˆfu 1.8 1.8 0.87 0.88 0.82 0.84 0.82 0.83\n(b) β = 0.2\nTable 3: Thresholdˆfu for different f j and ft .\nPr(yt = 1)= Pr(v = t) ·Pr(yt = 1|v = t)\n+ Pr(v ̸= t) ·Pr(yt = 1|v ̸= t)\n+ Pr(fake) ·Pr(yt = 1|fake) (30)\n= nf t\nn + m ·p + n(1 − ft )\nn + m ·q + m\nn + m , (31)\nPr(y j = 1|yt = 1)= Pr(y j = yt = 1)\nPr(yt = 1) (32)\n=q +\nf j q(p −q)+ β\n1−β ·( l\nd−1 −q)\nft p +( 1 − ft )q + β\n1−β\n. (33)\nGiven a non-target item u ̸= j, we have the following:\nPr(y j = yu = 1)\n= Pr(v = u) ·Pr(y j = yu = 1|v = u)\n+ Pr(v = j) ·Pr(y j = yu = 1|v = j)\n+ Pr(v ̸= j,u) ·Pr(y j = yu = 1|v ̸= j,u)\n+ Pr(fake) ·Pr(y j = yu = 1|fake) (34)\n= nf u\nn + m ·pq + nf j\nn + m ·pq + n(1 − fu − f j )\nn + m ·q2\n+ m\nn + m · l\nd −1 · l −1\nd −2 , (35)\nPr(yu = 1)\n= Pr(v = u) ·Pr(yu = 1|v = u)\n+ Pr(v ̸= u) ·Pr(yu = 1|v ̸= u)\n+ Pr(fake) ·Pr(yu = 1|fake) (36)\n= nf u\nn + m ·p + n(1 − fu)\nn + m ·q + m\nn + m · l\nd −1 , (37)\nPr(y j = 1|yu = 1)\n= Pr(y j = yu = 1)\nPr(yu = 1) (38)\n= q +\nf j q(p −q)+ β\n1−β · l\nd−1 ·( l−1\nd−2 −q)\nfu p +( 1 − fu)q + β\n1−β · l\nd−1\n. (39)\nSuppose both t and u are among the top-N items with the\nlargest estimated frequencies. The true frequency ft for the\ntarget item t is small, since our attack aims to promote an\nunpopular item. We havePr(y j = 1|yt = 1) < Pr(y j = 1|yu =\n1) when fu is smaller than a threshold ˆfu. Table 3 shows such\nthreshold for different values of f j and ft , where β = 0.05\nand β = 0.2. We observe that fu is highly likely smaller than"
    },
    {
      "page_number": 13,
      "text": "kRR OUE OLH\nNo Norm No Norm Detect Both No Norm Detect Both\nRPA 2e-3 -1e-3 0.50 2e-3 0.50 2e-3 -2e-3 -2e-3 -2e-3 -2e-3\nRIA 0.05 -4e-3 0.05 0.03 – – 0.05 0.03 – –\nMGA 2.72 0.43 1.58 0.46 7e-17 -2e-16 1.18 0.43 1.18 0.43\nTable 4: Overall gains of the three attacks on the IPUMS\ndataset after countermeasures are deployed. The column\n“No” means no countermeasure is used. The column\n“Both” means the combined countermeasure. “–” means\nthat the countermeasure is not applicable. Only normal-\nization is applicable for kRR.\nthe threshold ˆfu for a variety of f j when β = 0.2,a s ˆfu is\nvery large (sometimes even larger than 1). This observation\nshows that if we randomly pick an item as j and compare\nthe conditional probabilities Pr(y j = 1|yu = 1) for each item\nu in the top- N items, then we can detect the item with the\nsmallest conditional probability as the target item. However,\nwhen β = 0.05, the effectiveness of such detection method\ndepends on the true frequencies f j and fu.\nOLH: The conditional probability based detection method\ncan also be used for OLH when r = 1. Speciﬁcally, we can\nconstruct a d-bit binary vector yyy for each user whose vth entry\nyv = 1 if and only if H(v)= a, where (H,a) is the user’s\nperturbed value. Assuming the hash function hashes an item\nuniformly at random to a hash value in [d′]. Then, we have\nthe following conditional probabilities:\nPr(y j = 1|yt = 1)= q + f j q(p −q)\nft p +( 1 − ft )q + β\n1−β\n, (40)\nPr(y j = 1|yu = 1)= q + f j q(p −q)\nfu p +( 1 − fu)q + β\n1−β ·q\n. (41)\n6.4 Experimental Results\nWe empirically evaluate the effectiveness of the three coun-\ntermeasures. Unless otherwise mentioned, we focus on nor-\nmalization and detecting fake users as the conditional proba-\nbility based detection is only applicable for one target item.\nNote that normalization and detecting fake users can also be\nused together. Speciﬁcally, the central server can ﬁrst detect\nand remove the fake users, and then perform normalization.\nTherefore, we will also evaluate the combined countermea-\nsure. We use the same default experimental setup as those\nin Section 5.1. Moreover, we use the FP-growth algorithm\nimplemented in the Python package mlxtend [46] to identify\nfrequent itemsets.\n6.4.1 Frequency Estimation\nOverall results: Table 4 shows the experimental results with\nno countermeasure, normalization, detection, and combined\ncountermeasure, where β = 0.05 and r = 10. We observe that\nthe countermeasures are effective in some scenarios. For ex-\nample, for OUE, combining the two countermeasures leads to\nan overall gain of -2e-16 for MGA, which means that the esti-\nmated total frequency of the target items is even smaller than\nthe one before attack. However, the countermeasures are inef-\nfective in other scenarios. For instance, MGA can still achieve\na large overall gain of 0.43 for OLH even if both countermea-\nsures are used. Normalization can reduce the overall gains\nof all the three attacks for the three protocols except RPA\nfor OLH. However, MGA still achieves large overall gains\nafter normalization. Detecting fake users is ineffective for\nRPA because RPA randomly samples perturbed values in the\nencoded space for the fake users and thus the perturbed values\ndo not have meaningful statistical patterns. When the counter-\nmeasures are used, MGA is still the most effective attack in\nmost cases. Therefore, we focus on MGA and further study\nthe impact of β and r on the countermeasure effectiveness.\nImpact ofβ and r on MGA: Figure 7a-7b show the impact\nof β on the countermeasures against MGA when we ﬁxr = 10,\nwhile Figure 7c-7d show the results for r when we ﬁx β =\n0.05 on the IPUMS dataset. First, we observe that for OUE,\ndetecting fake users and the combined countermeasure can\neffectively defend against the MGA attacks (i.e., reduce the\noverall gains to almost 0) when β and r are larger than some\nthresholds, e.g., β > 0.001 and r ≥3. The countermeasures\nare ineffective when β or r is small (e.g., β ≤0.001 or r ≤2).\nThis is because the detection method relies on that the target\nitemset is frequent and abnormal, but the target itemset is not\nfrequent when β is small and is not abnormal among the users’\nperturbed values when r is small.\nSecond, for OLH, detecting fake users and the combined\ncountermeasure can effectively defend against the MGA at-\ntacks only when r is not too small nor large, e.g., 3 ≤r ≤5\nin our experiments. Recall that, to attack OLH, our MGA\nrandomly samples 1,000 hash functions and uses the one that\nhashes the largest number of target items to the same value\nfor each fake user. When r ≤ 5, our MGA can ﬁnd a hash\nfunction that hashes all target items to the same value. There-\nfore, the target itemset is frequent among the users’ perturbed\nvalues. Moreover, when r ≥3, the frequent target itemset is\nalso abnormal. As a result, the detection method can detect\nMGA when 3 ≤r ≤5. When r ≥6, our MGA can only ﬁnd\na hash function among the 1,000 random ones that hashes a\nsubset of the target items to the same value for each fake user.\nIn other words, each fake user essentially randomly picks a\nsubset of the target items and promotes them. Therefore, the\nentire target itemset is not frequent enough and MGA evades\ndetection. Our MGA evades detection for all the explored β\nin Figure 7b because r = 10 in these experiments.\nAdaptive MGA to OUE: Inspired by the evasiveness of\nMGA to OLH, we can also adapt MGA to OUE that evades\ndetection. Speciﬁcally, for each fake user, instead of using a\nperturbed value that supports all r target items, we randomly\nselect r′of the r target items and ﬁnd a perturbed value that"
    },
    {
      "page_number": 14,
      "text": "(a) OUE (b) OLH (c) OUE (d) OLH (e) Adaptive MGA\nFigure 7: (a)-(b) Impact ofβ on the countermeasures against MGA whenr = 10. (c)-(d) Impact ofr on the countermea-\nsures against MGA whenβ = 0.05. (e) Impact ofr′on the adaptive MGA (MGA-A) to OUE whenr = 10.\n(a) N (b) β\nFigure 8: Impact ofN and β on the detection rate of the\nconditional probability based method forr = 1.\nsupports the r′selected target items. The adaptive attack splits\nthe frequency of the target itemset with size r to\n(r\nr′\n)\nitemsets\nwith size r′, which becomes much harder to detect. We call\nsuch adaptive attacks MGA-A. Figure 7e shows the impact\nof r′on MGA-A to OUE when r = 10. We observe that our\nadaptive MGA achieves smaller overall gains asr′becomes\nsmaller when no countermeasures are deployed. However,\nour adaptive MGA evades detection when r′< r.\nAttack stealthiness: If the frequent itemset mining based\ndetection method returns an abnormal frequent itemset, then\nthe central server predicts that it is under our MGA attack.\nOur attack is stealthy if the central server cannot detect it. Our\nresults show that, for OUE, our MGA is stealthy when β or r\nis small (e.g., β ≤0.001 or r ≤2), and our adaptive MGA is\nstealthy when r′< r. For OLH, our MGA is stealthy whenr is\nsmall or large enough, e.g.,r ≤2 or r ≥6 in our experiments.\nConditional probability based detection for r = 1: We\nmeasure the effectiveness of the conditional probability based\ndetection method using detection rate. Speciﬁcally, in each\nexperiment, we perform our MGA attack with a random target\nitem 50 times and the detection rate is the fraction of the 50\nexperiment trials in which the target item is correctly detected.\nFigure 8a shows the impact of N on the detection rate when\nwe ﬁx β = 0.05 on the IPUMS dataset. We observe that the\ndetection rate ﬁrst increases and then decreases as N grows.\nThis is because when N is too small, e.g., N = 1, the target\nitem is likely not in the top-N items; and when N is too large,\nit’s more likely that there exists a non-target item in the top-N\nitems that has a smaller conditional probability than the target\nitem. We notice that the detection rate is lower for OLH than\nfor OUE. This is because the threshold ˆfu for OLH is smaller\nthan that for OUE, e.g., ˆfu = 0.18 for OLH and ˆfu = 0.26 for\nOUE when ft = f j = 0.01. Figure 8b shows the impact of β\non the detection rate, where we explore N = 1 to 20 to ﬁnd\nthe N that achieves the highest detection rate for each given\nβ. We observe that the detection rate increases as β increases,\nwhich implies that the MGA attack with r = 1 is easier to\ndetect when there are more fake users. Once the target item\nis detected, the server can compute the sum of the estimated\nfrequencies of all non-target items as ˜fU = ∑u̸=t ˜fu and set the\nestimated frequency of the target item as ˜ft = 1 − ˜fU , which\ncan reduce the overall gain of MGA. For instance, the overall\ngain decreases from 2.37 to 0.095 for OLH when β = 0.1.\n6.4.2 Heavy Hitter Identiﬁcation\nNormalization is ineffective for heavy hitter identiﬁcation\nbecause normalization does not impact the ranking of the\nitems’ estimated frequencies. Moreover, the conditional prob-\nability based detection is only applicable to one target item.\nTherefore, we perform experiments on detecting fake users\nfor heavy hitter identiﬁcation. Moreover, we focus on MGA\nbecause RIA and RPA are ineffective even without detecting\nfake users (see Figures 4). We observe that detecting fake\nusers is effective in some scenarios but not in others. For\ninstance, when\nr = 5, detecting fake users can reduce the\nsuccess rate of MGA from 1 to 0, as all fake users can be\ndetected. However, when r = 10, our MGA can still achieve\na success rate of 1.\n6.5 Other Countermeasures\nDetecting fake users is related to Sybil detection in dis-\ntributed systems and social networks. Many methods have\nbeen proposed to mitigate Sybil attacks. For instance, meth-\nods [12, 16, 26, 52, 56, 57, 67, 68] that leverage content, be-\nhavior, and social graphs are developed to detect fake users\nin social networks. Our detection method can be viewed as\na content-based method. Speciﬁcally, our detection method\nanalyzes the statistical patterns of the user-generated content\n(i.e., perturbed values sent to the central server) to detect fake\nusers. However, our detection method is different from the\ncontent-based methods to detect fake users in social networks,\nas the user-generated content and their statistical patterns dif-"
    },
    {
      "page_number": 15,
      "text": "fer. Social-graph-based methods are inapplicable when the\nsocial graphs are not available.\nAnother countermeasure is to leverage Proof-of-Work [20],\nlike how Sybil is mitigated in Bitcoin. In particular, before\na user can participate in the LDP protocol, the central server\nsends a random string to the user; and the user is allowed to\nparticipate the LDP protocol after the user ﬁnds a string such\nthat the cryptographic hash value of the concatenated string\nhas a certain property, e.g., the ﬁrst 32 bits are all 0. However,\nsuch method incurs a large computational cost for genuine\nusers, which impacts user experience. Moreover, when users\nuse mobile devices such as phones and IoT devices, it is chal-\nlenging for them to perform the Proof-of-Work. Malicious-\nparty-resistant SMPC could also be used to limit the impact\nof fake users (e.g., [40]). However, such methods generally\nsacriﬁce computational efﬁciency.\n7 Discussion\nApplicability to shufﬂing-based and SMPC-based proto-\ncols: Shufﬂing-based protocols [21] apply shufﬂing to the\nusers’ perturbed vectors such that a better DP guarantee can\nbe derived. Since they still encode and perturb each user’s\ndata, our attacks are applicable. When SMPC-based proto-\ncols have local encoding and perturbation steps like [34], our\nattacks are applicable and the security-privacy trade-off still\nholds. When there is no local encoding or perturbation step in\nthe SMPC-based DP protocols like [48], our RPA and MGA\nare not applicable because an attacker cannot manipulate the\nperturbed vectors. However, our RIA is still applicable be-\ncause it only needs to modify the item value. In this case, we\ndo not have the security-privacy trade-off because the overall\ngain of RIA does not rely on the privacy budget.\nRIA without perturbation: A variant of RIA is that a fake\nuser samples one of the r target items randomly, encodes it,\nand sends the encoded value to the central server without\nperturbing it. When\nr = 1, this RIA variant has the same\noverall gain as MGA. Whenr > 1, the RIA variant uses a fake\nuser to promote only one target item. However, MGA uses\na fake user to simultaneously promote multiple target items,\nwhich means that its overall gain is multiple times of the RIA\nvariant’s overall gain. Moreover, it may be easy for the central\nserver to detect the RIA variant for OUE. Speciﬁcally, the\nserver can count the number of 1’s in a vector from a user. If\nthere is only one entry that is 1, then it is likely that the vector\nis from a fake user as the probability that a genuine vector\ncontains a single 1 is fairly small.\nDefending OLH by restricting the hash functions:Since\nMGA to OLH relies on searching a hash function that maps\ntarget items to the same hash value, the server could restrict\nthe space of seeds of the hash function or select the hash\nfunction by itself to defend OLH against MGA. However, the\ndefense may break the privacy guarantees. In particular, an\nuntrusted server could carefully select a space of seeds or a\nhash function that does not have collisions in the item domain.\nFor instance, a hash value h corresponds to a unique item.\nWhen receiving a hash value h from a user, the server knows\nthe user’s item, which breaks the LDP guarantee.\n8 Conclusion\nIn this work, we perform the ﬁrst systematic study on data\npoisoning attacks to LDP protocols. Our results show that\nan attacker can inject fake users to an LDP protocol and\nsend carefully crafted data to the server such that the target\nitems are estimated to have high frequencies or promoted as\nheavy hitters. We show that we can formulate such an attack\nas an optimization problem, solving which an attacker can\nmaximize its attack effectiveness. We theoretically and/or\nempirically show the effectiveness of our attacks. Moreover,\nwe explore three countermeasures against our attacks. Our\nempirical results show that these countermeasures have lim-\nited effectiveness in some scenarios, highlighting the needs\nfor new defenses against our attacks.\nInteresting future work includes generalizing our attacks\nto other LDP protocols, e.g., LDP protocols for itemset min-\ning [61] and key-value pairs [66], as well as developing new\ndefenses to mitigate our attacks.\nAcknowledgements\nWe thank the anonymous reviewers for their constructive com-\nments. The conditional probability based detection method\nfor one target item was suggested by a reviewer. This work\nwas supported by NSF grant No.1937786.\nReferences\n[1] Equifax Announces Cybersecurity Incident Involving\nConsumer Information. http://bit.ly/2PEHuPk, 2017.\n[2] A hacker gained access to 100 million Capital One credit\ncard applications and accounts. https://cnn.it/2WINTKV,\n2019.\n[3] In systemic breach, hackers steal millions of Bulgarians’\nﬁnancial data. https://reut.rs/2r6sMq3, 2019.\n[4] San francisco ﬁre department calls for service.\nhttp://bit.ly/336sddL, 2019.\n[5] Milton Abramowitz and Irene A Stegun. Handbook\nof mathematical functions: with formulas, graphs, and\nmathematical tables. Courier Corporation, 1965.\n[6] Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami.\nMining association rules between sets of items in large\ndatabases. In SIGMOD, 1993."
    },
    {
      "page_number": 16,
      "text": "[7] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poi-\nsoning attacks against autoregressive models. In AAAI,\n2016.\n[8] Brendan Avent, Aleksandra Korolova, David Zeber,\nTorgeir Hovden, and Benjamin Livshits. BLENDER:\nEnabling local search with a hybrid differential privacy\nmodel. In USENIX Security, 2017.\n[9] Raef Bassily, Kobbi Nissim, Uri Stemmer, and\nAbhradeep Guha Thakurta. Practical locally private\nheavy hitters. In NeurIPS, 2017.\n[10] Raef Bassily and Adam Smith. Local, private, efﬁcient\nprotocols for succinct histograms. In STOC, 2015.\n[11] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poi-\nsoning attacks against support vector machines. In\nICML, 2012.\n[12] Qiang Cao, Xiaowei Yang, Jieqi Yu, and Christopher\nPalow. Uncovering large groups of active malicious\naccounts in online social networks. In CCS, 2014.\n[13] Albert Cheu, Adam Smith, and Jonathan Ullman. Ma-\nnipulation attacks in local differential privacy. arXiv,\n2019.\n[14] Yann Collet. xxhash: Extremely fast hash algorithm.\nhttps://github.com/Cyan4973/xxHash, 2016.\n[15] Graham Cormode, Tejas Kulkarni, and Divesh Srivas-\ntava. Marginal release under local differential privacy.\nIn SIGMOD, 2018.\n[16] George Danezis and Prateek Mittal. Sybilinfer: Detect-\ning sybil nodes using social networks. In NDSS, 2009.\n[17] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin.\nCollecting telemetry data privately. In NeurIPS, 2017.\n[18] John C Duchi, Michael I Jordan, and Martin J Wain-\nwright. Local privacy and statistical minimax rates. In\nFOCS, 2013.\n[19] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. Calibrating noise to sensitivity in private\ndata analysis. In TCC, 2006.\n[20] Cynthia Dwork and Moni Naor. Pricing via processing\nor combatting junk mail. In CRYPTO, 1992.\n[21] Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth\nRaghunathan, Kunal Talwar, and Abhradeep Thakurta.\nAmpliﬁcation by shufﬂing: From local to central differ-\nential privacy via anonymity. In SODA, 2019.\n[22] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova.\nRappor: Randomized aggregatable privacy-preserving\nordinal response. In CCS, 2014.\n[23] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil\nGong. Local model poisoning attacks to byzantine-\nrobust federated learning. In USENIX Security, 2020.\n[24] Minghong Fang, Neil Zhenqiang Gong, and Jia Liu. In-\nﬂuence function based data poisoning attacks to top-n\nrecommender systems. In WWW, 2020.\n[25] Minghong Fang, Guolei Yang, Neil Zhenqiang Gong,\nand Jia Liu. Poisoning attacks to graph-based recom-\nmender systems. In ACSAC, 2018.\n[26] Neil Zhenqiang Gong, Mario Frank, and Prateek Mittal.\nSybilbelief: A semi-supervised learning approach for\nstructure-based sybil detection. TIFS, 2014.\n[27] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Sid-\ndharth Garg. Badnets: Evaluating backdooring attacks\non deep neural networks. IEEE Access, 2019.\n[28] Ling Huang, Anthony D Joseph, Blaine Nelson, Ben-\njamin IP Rubinstein, and J Doug Tygar. Adversarial\nmachine learning. In AISec, 2011.\n[29] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang\nLiu, Cristina Nita-Rotaru, and Bo Li. Manipulating ma-\nchine learning: Poisoning attacks and countermeasures\nfor regression learning. In S&P, 2018.\n[30] Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong.\nIntrinsic certiﬁed robustness of bagging against data\npoisoning attacks. AAAI, 2021.\n[31] Jinyuan Jia and Neil Zhenqiang Gong. Calibrate: Fre-\nquency estimation and heavy hitter identiﬁcation with\nlocal differential privacy via incorporating prior knowl-\nedge. In INFOCOM, 2019.\n[32] Peter Kairouz, Keith Bonawitz, and Daniel Ramage. Dis-\ncrete distribution estimation under local privacy. In\nICML, 2016.\n[33] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.\nExtremal mechanisms for local differential privacy. In\nNeurIPS, 2014.\n[34] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.\nSecure multi-party differential privacy. In NeurIPS,\n2015.\n[35] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy V orob-\neychik. Data poisoning attacks on factorization-based\ncollaborative ﬁltering. In NeurIPS, 2016.\n[36] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee,\nJuan Zhai, Weihang Wang, and Xiangyu Zhang. Trojan-\ning attack on neural networks. In NDSS, 2018."
    },
    {
      "page_number": 17,
      "text": "[37] Shike Mei and Xiaojin Zhu. Using machine teaching to\nidentify optimal training-set attacks on machine learners.\nIn AAAI, 2015.\n[38] Mehran Mozaffari-Kermani, Susmita Sur-Kolay, Anand\nRaghunathan, and Niraj K Jha. Systematic poisoning\nattacks on and defenses for machine learning in health-\ncare. IEEE journal of biomedical and health informatics,\n2014.\n[39] Luis Muñoz-González, Battista Biggio, Ambra Demon-\ntis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu,\nand Fabio Roli. Towards poisoning of deep learning al-\ngorithms with back-gradient optimization. In AISec,\n2017.\n[40] Moni Naor, Benny Pinkas, and Eyal Ronen. How to\n(not) share a password: Privacy preserving protocols for\nﬁnding heavy hitters with adversarial behavior. In CCS,\n2019.\n[41] Blaine Nelson, Marco Barreno, Fuching Jack Chi, An-\nthony D Joseph, Benjamin IP Rubinstein, Udam Saini,\nCharles A Sutton, J Doug Tygar, and Kai Xia. Exploit-\ning machine learning to subvert your spam ﬁlter. LEET,\n2008.\n[42] Andrew Newell, Rahul Potharaju, Luojie Xiang, and\nCristina Nita-Rotaru. On the practicality of integrity\nattacks on document-level sentiment analysis. In AISec,\n2014.\n[43] James Newsome, Brad Karp, and Dawn Song. Para-\ngraph: Thwarting signature learning by training mali-\nciously. In RAID workshop, 2006.\n[44] Roberto Perdisci, David Dagon, Wenke Lee, Prahlad\nFogla, and Monirul Sharif. Misleading worm signature\ngenerators using deliberate noise injection. In S&P,\n2006.\n[45] Zhan Qin, Yin Yang, Ting Yu, Issa Khalil, Xiaokui Xiao,\nand Kui Ren. Heavy hitter estimation over set-valued\ndata with local differential privacy. In CCS, 2016.\n[46] Sebastian Raschka. Mlxtend: Providing machine learn-\ning and data science utilities and extensions to python’s\nscientiﬁc computing stack. The Journal of Open Source\nSoftware, 2018.\n[47] Xuebin Ren, Chia-Mu Yu, Weiren Yu, Shusen Yang,\nXinyu Yang, Julie A McCann, and S Yu Philip. LoPub:\nHigh-dimensional crowdsourced data publication with\nlocal differential privacy. TIFS, 2018.\n[48] Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ash-\nwin Machanavajjhala, and Somesh Jha. Cryptε: Crypto-\nassisted differential privacy on untrusted servers. In\nSIGMOD, 2020.\n[49] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang,\nAnthony D Joseph, Shing-hon Lau, Satish Rao, Nina\nTaft, and J Doug Tygar. Antidote: understanding and\ndefending against poisoning of anomaly detectors. In\nIMC, 2009.\n[50] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian\nSuciu, Christoph Studer, Tudor Dumitras, and Tom Gold-\nstein. Poison frogs! targeted clean-label poisoning at-\ntacks on neural networks. In NeurIPS, 2018.\n[51] Ruggles Steven, Flood Sarah, Goeken Ronald, Grover\nJosiah, Meyer Erin, Pacas Jose, and Sobek Matthew.\nIpums usa: Version 9.0 [dataset]. minneapolis, mn:\nIpums, 2019. https://doi.org/10.18128/D010.V9.0,\n2019.\n[52] Gianluca Stringhini, Christopher Kruegel, and Giovanni\nVigna. Detecting spammers on social networks. In\nACSAC, 2010.\n[53] Apple Differential Privacy Team. Learning with privacy\nat scale. Machine Learning Journal, 2017.\n[54] Kurt Thomas, Damon McCoy, Chris Grier, Alek Kolcz,\nand Vern Paxson. Trafﬁcking fraudulent accounts: The\nrole of the underground market in twitter spam and\nabuse. In USENIX Security, 2013.\n[55] Binghui Wang and Neil Zhenqiang Gong. Attacking\ngraph-based classiﬁcation via manipulating the graph\nstructure. In CCS, 2019.\n[56] Binghui Wang, Jinyuan Jia, and Neil Zhenqiang Gong.\nGraph-based security and privacy analytics via collec-\ntive classiﬁcation with joint weight learning and propa-\ngation. In NDSS, 2019.\n[57] Gang Wang, Tristan Konolige, Christo Wilson, Xiao\nWang, Haitao Zheng, and Ben Y Zhao. You are how\nyou click: Clickstream analysis for sybil detection. In\nUSENIX Security, 2013.\n[58] Gang Wang, Tianyi Wang, Haitao Zheng, and Ben Y\nZhao. Man vs. machine: Practical adversarial detec-\ntion of malicious crowdsourcing workers. In USENIX\nSecurity, 2014.\n[59] Tianhao Wang, Jeremiah Blocki, Ninghui Li, and\nSomesh Jha. Locally differentially private protocols\nfor frequency estimation. In USENIX Security, 2017.\n[60] Tianhao Wang, Bolin Ding, Jingren Zhou, Cheng Hong,\nZhicong Huang, Ninghui Li, and Somesh Jha. Answer-\ning multi-dimensional analytical queries under local dif-\nferential privacy. In SIGMOD, 2019."
    },
    {
      "page_number": 18,
      "text": "[61] Tianhao Wang, Ninghui Li, and Somesh Jha. Locally\ndifferentially private frequent itemset mining. In S&P,\n2018.\n[62] Tianhao Wang, Ninghui Li, and Somesh Jha. Locally\ndifferentially private heavy hitter identiﬁcation. TDSC,\n2019.\n[63] Tianhao Wang, Milan Lopuhaä-Zwakenberg, Zitao Li,\nBoris Skoric, and Ninghui Li. Locally differentially\nprivate frequency estimation with consistency. In NDSS,\n2020.\n[64] Stanley L Warner. Randomized response: A survey\ntechnique for eliminating evasive answer bias. Journal\nof the American Statistical Association, 1965.\n[65] Guolei Yang, Neil Zhenqiang Gong, and Ying Cai. Fake\nco-visitation injection attacks to recommender systems.\nIn NDSS, 2017.\n[66] Qingqing Ye, Haibo Hu, Xiaofeng Meng, and Huadi\nZheng. Privkv: Key-value data collection with local\ndifferential privacy. In S&P, 2019.\n[67] Haifeng Yu, Haifeng Yu, Michael Kaminsky, Phillip B\nGibbons, and Abraham Flaxman. Sybilguard: defending\nagainst sybil attacks via social networks. In SIGCOMM,\n2006.\n[68] Dong Yuan, Yuanli Miao, Neil Zhenqiang Gong, Zheng\nYang, Qi Li, Dawn Song, Qian Wang, and Xiao Liang.\nDetecting fake accounts in online social networks at the\ntime of registrations. In CCS, 2019.\n[69] Zhikun Zhang, Tianhao Wang, Ninghui Li, Shibo He,\nand Jiming Chen. Calm: Consistent adaptive local\nmarginal for marginal release under local differential\nprivacy. In CCS, 2018.\nA Proof of Theorem 2\nProof. Let β(1 − fT )+ β(d−r)\neε−1 > β(2r − fT )+ 2βr\neε−1 ,w eh a v e :\n1 + d −r\neε −1 > 2r + 2r\neε −1 ⇐⇒ d −3r\neε −1 > 2r −1. (42)\nSince eε > 1, the inequality above is equivalent to d > (2r −\n1)(eε −1)+ 3r.\nB FPRs of Detecting Fake Users\nOUE: If a user’s perturbed binary vector yyy follows the\nOUE protocol, then we can calculate the probability that the\nitems in a set B of size z, are all 1 in the perturbed binary\nvector as follows: Pr(yb = 1,∀b ∈ B)= pqz−1 if v ∈ B and\nPr(yb = 1,∀b ∈B)= qz otherwise, where yb is the bth bit of\nthe perturbed binary vector yyy and v is the user’s item. Let\nfB = ∑b∈B fb denote the sum of true frequencies of all items\nin B, X1 denote the random variable representing the number\nof users whose items are in B and whose perturbed binary\nvectors are 1 for all items inB, and X2 denote the random vari-\nable representing the number of users whose items are not in\nB and whose perturbed binary vectors are 1 for all items in B.\nIf all the n + m users follow the OUE protocol, then we have\nthe following distributions: X1 ∼ Binom( fB(n + m), pqz−1)\nand X2 ∼Binom((1− fB)(n +m),qz), where Binom is a bino-\nmial distribution. Now we consider another random variable\nX = X1 + X2, which represents the number of users whose\nperturbed binary vectors are 1 for all items in B. X follows a\ndistribution with mean μ and variance Var as follows:\nμ = fB(n + m)pqz−1 +( 1 − fB)(n + m)qz (43)\n≤(n + m)pqz−1 (44)\nVar = fB(n + m)pqz−1(1 −pqz−1)\n+( 1 − fB)(n + m)qz(1 −qz) (45)\n≤(n + m)pqz−1(1 −pqz−1). (46)\nBased on the Chebyshev’s inequality, for any τz > (n +\nm)pqz−1,w eh a v e :\nPr(X ≥τz)= Pr(X −μ ≥τz −μ)\n≤Pr(|X −μ|≥ τz −μ)\n≤ Var\n(τz −μ)2\n≤ (n + m)pqz−1(1 −pqz−1)\n[τz −(n + m)pqz−1]2 (47)\nHere, if we choose τz as the threshold, the probability Pr(X ≥\nτz) is the false positive rate, which is upper bounded by\n(n+m)pqz−1(1−pqz−1)\n[τz−(n+m)pqz−1]2 .\nOLH: As discussed in Section 6.2, we ﬁrst construct a d-bit\nbinary vector yyy for each user with a tuple (H,a) such that\nyv = 1 if and only if H(v)= a. For an item set B of size z,\nassume X is a random variable that represents the number of\nusers whose constructed binary vectors are 1’s for all items\nin B. If all the n + m users follow the OLH protocol, then for\nany τz > 0, the probability that X ≥τz is bounded as follows:\nPr(X ≥τz)= 1 −Pr(X ≤τz −1)\n= 1 −I(1 −qz−1;n + m −τz + 1,τz)\n= I(qz−1;τz,n + m −τz + 1) (48)\nNote that if we set τz as the threshold, the probability Pr(X ≥\nτz) is the false positive rate."
    }
  ],
  "full_text": "Data Poisoning Attacks to Local Differential Privacy Protocols\nXiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong\nDuke University\n{xiaoyu.cao, jinyuan.jia, neil.gong}@duke.edu\nAbstract\nLocal Differential Privacy (LDP) protocols enable an un-\ntrusted data collector to perform privacy-preserving data an-\nalytics. In particular, each user locally perturbs its data to\npreserve privacy before sending it to the data collector, who\naggregates the perturbed data to obtain statistics of interest. In\nthe past several years, researchers from multiple communities–\nsuch as security, database, and theoretical computer science–\nhave proposed many LDP protocols. These studies mainly fo-\ncused on improving the utility of the LDP protocols. However,\nthe security of LDP protocols is largely unexplored.\nIn this work, we aim to bridge this gap. We focus on LDP\nprotocols for frequency estimation and heavy hitter identiﬁ-\ncation, which are two basic data analytics tasks. Speciﬁcally,\nwe show that an attacker can inject fake users into an LDP\nprotocol and the fake users send carefully crafted data to the\ndata collector such that the LDP protocol estimates high fre-\nquencies for arbitrary attacker-chosen items or identiﬁes them\nas heavy hitters. We call our attacksdata poisoning attacks.\nWe theoretically and/or empirically show the effectiveness of\nour attacks. We also explore three countermeasures against\nour attacks. Our experimental results show that they can effec-\ntively defend against our attacks in some scenarios but have\nlimited effectiveness in others, highlighting the needs for new\ndefenses against our attacks.\n1 Introduction\nVarious data breaches [1–3] have highlighted the challenges\nof relying on a data collector (e.g., Equifax) to protect users’\nprivate data. Local Differential Privacy (LDP), a variant of\ndifferential privacy [19], aims to address such challenges. In\nparticular, an LDP protocol encodes and perturbs a user’s data\nto protect privacy before sending it to the data collector, who\naggregates the users’ perturbed data to obtain statistics of\ninterest. Therefore, even if the data collector is compromised,\nuser privacy is still preserved as the attacker only has access\nto users’ privacy-preserving perturbed data. Because of the re-\nsilience against untrusted data collectors, LDP has attracted in-\ncreasing attention in both academia and industry. Speciﬁcally,\nmany LDP protocols [8\n–10,15,18,22,31–33,45,47,59–63,69]\nhave been developed in the past several years. Moreover, some\nof these protocols have been widely deployed in industry in-\ncluding but not limited to Google, Microsoft, and Apple. For\ninstance, Google deployed LDP [22] in the Chrome browser to\ncollect users’ default homepages for Chrome; Microsoft [17]\nintegrated LDP in Windows 10 to collect application usage\nstatistics; and Apple [53] adopted LDP on iOS to identify pop-\nular emojis, which are subsequently recommended to users.\nSince LDP perturbs each user’s data, it sacriﬁces utility of\nthe data analytics results obtained by the data collector. There-\nfore, existing studies on LDP mainly focused on improving\nthe utility via designing new methods to encode/perturb users’\ndata and aggregate the perturbed data to derive statistical\nresults. However, the security of LDP is largely unexplored.\nIn this work, we aim to bridge this gap. In particular, we\npropose a family of attacks called data poisoning attacksto\nLDP protocols. In our attacks, an attacker injects fake users\nto an LDP protocol and carefully crafts the data sent from the\nfake users to the data collector, with the goal to manipulate the\ndata analytics results as the attacker desires. Speciﬁcally, we\nfocus on LDP protocols for Frequency Estimationand Heavy\nHitter Identiﬁcation, which are two basic data analytics tasks\nand are usually the ﬁrst step towards more advanced tasks.\nThe goal of frequency estimation is to estimate the fraction\nof users (i.e., frequency) that have a certain item for each of\na set of items, while the goal of heavy hitter identiﬁcation\nis to only identify the top-k items that are the most frequent\namong the users without estimating the items’ frequencies.\nOur attacks can increase the estimated frequencies for arbi-\ntrary attacker-chosen items (called target items) in frequency\nestimation or promote them to be identiﬁed as top-k heavy hit-\nters in heavy hitter identiﬁcation. Our attacks result in severe\nsecurity threats to LDP-based data analytics. For example,\nan attacker can promote a phishing webpage as a popular\ndefault homepage of Chrome; an attacker can increase the\nestimated popularity of its (malicious) application when LDP\n\nis used to estimate application popularity; and an attacker can\nmanipulate the identiﬁed and recommended popular emojis,\nresulting in bad user experience and frustration.\nThe major challenge of data poisoning attacks is that, given\na limited number of fake users an attacker can inject, what data\nthe fake users should send to the data collector such that the\nattack effectiveness is maximized. To address the challenge,\nwe formulate our attacks as an optimization problem, whose\nobjective function is to maximize the attack effectiveness and\nwhose solution is the data that fake users should send to the\ndata collector. We call our optimization-based attackMaximal\nGain Attack (MGA). To better demonstrate the effectiveness\nof MGA, we also propose two baseline attacks in which the\nfake users send randomly crafted data to the data collector.\nThen, we apply our MGA and the baseline attacks to three\nstate-of-the-art LDP protocols for frequency estimation (i.e.,\nkRR [33], OUE [59], and OLH [59]) and one state-of-the-art\nLDP protocol for heavy hitter identiﬁcation (i.e., PEM [62]).\nWe theoretically evaluate the effectiveness of our attacks.\nSpeciﬁcally, we derive the frequency gainof the target items,\nwhich is the difference of the target items’ estimated frequen-\ncies after and before an attack. Our theoretical analysis shows\nthat our MGA can achieve the largest frequency gain among\npossible attacks. Our theoretical results also show a funda-\nmental security-privacy tradeoff for LDP protocols: when an\nLDP protocol provides higher privacy guarantees, the LDP\nprotocol is less secure against our attacks (i.e., the frequency\ngains are larger). Moreover, we observe that different LDP\nprotocols have different security levels against our attacks. For\ninstance, OUE and OLH have similar security levels against\nour attacks, and kRR is less secure than OUE and OLH when\nthe number of items is larger than a threshold. We also empir-\nically evaluate our attacks for both frequency estimation and\nheavy hitter identiﬁcation using a synthetic dataset and two\nreal-world datasets. Our empirical results also show the effec-\ntiveness of our attacks. For example, on all the three datasets,\nour MGA can promote 10 randomly selected target items to\nbe identiﬁed as top-15 heavy hitters when the attacker only\ninjects 5% of fake users.\nWe also explore three countermeasures, i.e., normalization,\ndetecting fake users, and detecting the target item, to defend\nagainst our attacks. Speciﬁcally, in normalization, the data\ncollector normalizes the estimated item frequencies to be a\nprobability distribution, i.e., each estimated item frequency is\nnon-negative and the estimated frequencies of all items sum\nto 1. Since our attacks craft the data for the fake users via\nsolving an optimization problem, the data from the fake users\nmay follow certain patterns that deviate from genuine users.\nTherefore, in our second countermeasure, the data collector\naims to detect fake users via analyzing the statistical patterns\nof the data from the users, and the data collector ﬁlters the\ndetected fake users before estimating frequencies or identify-\ning heavy hitters. The third countermeasure detects the target\nitem without detecting the fake users when there is only one\ntarget item. Our empirical results show that these counter-\nmeasures can effectively defend against our attacks in some\nscenarios. For example, when the attacker has 10 target items,\nnormalization can reduce the frequency gain of our MGA to\nOUE from 1.58 to 0.46 and detecting fake users can reduce\nthe frequency gain to be almost 0 because the data collector\ncan detect almost all fake users. However, our attacks are still\neffective in other scenarios. For instance, when the attacker\nhas 10 randomly selected target items, our MGA to OLH still\nachieves a frequency gain of 0.43 even if both detecting fake\nusers and normalization are used. Our results highlight the\nneeds for new defenses against our attacks.\nIn summary, our contributions are as follows:\n• We perform the ﬁrst systematic study on data poisoning\nattacks to LDP protocols for frequency estimation and\nheavy hitter identiﬁcation.\n• We show that, both theoretically and/or empirically, our\nattacks can effectively increase the estimated frequencies\nof the target items or promote them to be identiﬁed as\nheavy hitters.\n• We explore three countermeasures to defend against our\nattacks. Our empirical results highlight the needs for new\ndefenses against our attacks.\n2 Background and Related Work\nWe consider LDP protocols for two basic tasks, i.e.,frequency\nestimation [10, 18, 22, 31–33, 59, 63, 64, 69] and heavy hit-\nter identiﬁcation [9, 45, 62]. Suppose there are n users. Each\nuser holds one item from a certain domain, e.g., the default\nhomepage of a browser. We denote the domain of the items\nas {1,2,··· ,d}. For conciseness, we simplify {1,2,··· ,d}as\n[d]. In frequency estimation, the data collector (also calledcen-\ntral server) aims to estimate the frequency of each item among\nthe n users, while heavy hitter identiﬁcation aims to identify\nthe top-k items that have the largest frequencies among the n\nusers. Frequency of an item is deﬁned as the fraction of users\nwho have the item.\n2.1 Frequency Estimation\nAn LDP protocol for frequency estimation consists of three\nkey steps: encode, perturb, and aggregate. The encode step\nencodes each user’s item into some numerical value. We\ndenote the space of encoded values as D. The perturb step\nrandomly perturbs the value in the space D and sends the per-\nturbed value to the central server. The central server estimates\nitem frequencies using the perturbed values from all users in\nthe aggregate step. For simplicity, we denote by PE(v) the\nperturbed encoded value for an item v. Roughly speaking, a\nprotocol satisﬁes LDP if any two items are perturbed to the\nsame value with close probabilities. Formally, we have the\nfollowing deﬁnition:\n\nDeﬁnition 1(Local Differential Privacy). A protocolA sat-\nisﬁes ε-local differential privacy (ε-LDP) if for any pair of\nitems v1,v2 ∈ [d] and any perturbed valuey ∈ D, we have\nPr(PE(v1)= y) ≤eεPr(PE(v2)= y), where ε > 0 is called\nprivacy budget andPE(v) is the random perturbed encoded\nvalue of an item v.\nMoreover, an LDP protocol is called pure LDP if it satisﬁes\nthe following deﬁnition:\nDeﬁnition 2 (Pure LDP [59]). An LDP protocol is pure if\nthere are two probability parameters0 < q < p < 1 such that\nthe following equations hold for any pair of itemsv1,v2 ∈\n[d],v1 ̸= v2:\nPr(PE(v1) ∈{y|v1 ∈S(y)})= p (1)\nPr(PE(v2) ∈{y|v1 ∈S(y)})= q, (2)\nwhere S(y) is the set of items that y supports.\nWe note that the deﬁnition of the support S(y) depends on\nthe LDP protocol. For instance, for some LDP protocols [18,\n59], the support S(y) of a perturbed value y is the set of items\nwhose encoded values could be y. For a pure LDP protocol,\nthe aggregate step is as follows:\n˜fv =\n1\nn\nn\n∑\ni=1\n1S(yi )(v) −q\np −q , (3)\nwhere ˜fv is the estimated frequency for item v ∈[d], yi is the\nperturbed value from theith user, and1S(yi)(v) is an character-\nistic function, which outputs 1 if and only if yi supports item\nv. Formally, the characteristic function 1S(yi)(v) is deﬁned as\nfollows: 1S(y)(v) i s1i f v ∈S(y) and 0 otherwise.\nRoughly speaking, Equation (3) means that the frequency\nof an item is estimated as the fraction of users whose per-\nturbed values support the item normalized by p,q, and n.\nPure LDP protocols are unbiased estimators of the item fre-\nquencies [59], i.e., E[ ˜fv]= fv, where fv is the true frequency\nfor item v. Therefore, we have:\nn\n∑\ni=1\nE[1S(yi )(v)] =n( fv(p −q)+ q). (4)\nEquation (4) will be useful for the analysis of our attacks.\nNext, we describe three state-of-the-art pure LDP protocols,\ni.e., kRR [18], OUE [59], and OLH [59]. These three protocols\nare recommended for use in different scenarios. Speciﬁcally,\nkRR achieves the smallest estimation errors when the number\nof items is small, i.e., d < 3eε + 2. When the number of items\nis large, both OUE and OLH achieve the smallest estimation\nerrors. OUE has a larger communication cost, while OLH\nhas a larger computation cost for the central server. There-\nfore, when the communication cost is a bottleneck, OLH is\nrecommended, otherwise OUE is recommended.\n2.1.1 kRR\nEncode: kRR encodes an item v to itself. Therefore, the\nencoded space D for kRR is identical to the domain of items,\nwhich is D =[ d].\nPerturb: kRR keeps an encoded item unchanged with a\nprobability p and perturbs it to a different random item a ∈D\nwith probability q. Formally, we have:\nPr(y = a)=\n{\neε\nd−1+eε ≜ p, if a = v,\n1\nd−1+eε ≜ q, otherwise, (5)\nwhere y is the random perturbed value sent to the central\nserver when a user’s item is v.\nAggregate: The key for aggregation is to derive the support\nset. A perturbed value y only supports itself for kRR. Specif-\nically, we have S(y)= {y}. Given the support set, we can\nestimate item frequencies using Equation (3).\n2.1.2 OUE\nEncode: OUE encodes an item v to a d-bit binary vector eeev\nwhose bits are all zero except the v-th bit. The encoded space\nfor OUE is D = {0,1}d , where d is the number of items.\nPerturb: OUE perturbs the bits of the encoded binary vec-\ntor independently. Speciﬁcally, for each bit of the encoded\nbinary vector, if it is 1, then it remains 1 with a probability p.\nOtherwise if the bit is 0, it is ﬂipped to 1 with a probability q.\nFormally, we have:\nPr(yi = 1)=\n{\n1\n2 ≜ p, if i = v,\n1\neε+1 ≜ q, otherwise, (6)\nwhere the vector yyy =[ y1 y2 ··· yd ] is the perturbed value for\na user with item v.\nAggregate: A perturbed value yyy supports an item v if and\nonly if the v-th bit of yyy, denoted as yv, equals to 1. Formally,\nwe have S(yyy)= {v|v ∈[d] and yv = 1}.\n2.1.3 OLH\nEncode: OLH leverages a family of hash functionsH, each of\nwhich maps an item v ∈[d] to a value h ∈[d′], where d′< d.\nIn particular, OLH uses d′= eε + 1 as it achieves the best\nperformance [59]. An example of the hash function family\nH could be xxhash [ 14] with different seeds. Speciﬁcally,\na seed is a non-negative integer and each seed represents\na different xxhash hash function. In the encode step, OLH\nrandomly picks a hash function\nH from H. When xxhash\nis used, randomly picking a hash function is equivalent to\nrandomly selecting a non-negative integer as a seed. Then,\nOLH computes the hash value of the item v as h = H(v). The\ntuple (H,h) is the encoded value for the item v. The encoded\nspace for OLH is D = {(H,h)|H ∈H and h ∈[d′]}.\n\nPerturb: OLH only perturbs the hash value h and does not\nchange the hash function H. In particular, the hash value stays\nunchanged with probability p′ and switches to a different\nvalue in [d′] with probability q′. Formally, we have:\nPr(y =( H,a)) =\n{\neε\neε+d′−1 ≜ p′, if a = H(v),\n1\neε+d′−1 ≜ q′, otherwise, (7)\nwhere y is the perturbed value sent to the central server from a\nuser with item v. Therefore, the overall probability parameters\np and q are p = p′= eε\neε+d′−1 and q = 1\nd′·p′+(1− 1\nd′)·q′= 1\nd′.\nAggregate: A perturbed value y =( H,h) supports an item\nv ∈ [d] if v is hashed to h by H. Formally, we have S(y)=\n{v|v ∈[d] and H(v)= h}.\n2.2 Heavy Hitter Identiﬁcation\nThe goal of heavy hitter identiﬁcation [9, 10, 62] is to identify\nthe top-k items that are the most frequent among the n users.\nA direct and simple solution is to ﬁrst estimate the frequency\nof each item using a frequency estimation protocol and then\nselect the k items with the largest frequencies. However, such\nmethod is not scalable to a large number of items. In response,\na line of works [ 9, 10, 62] developed protocols to identify\nheavy hitters without estimating item frequencies. For ex-\nample, Bassily et al. [9] and Wang et al. [62] independently\ndeveloped a similar heavy hitter identiﬁcation protocol, which\ndivides users into groups and iteratively applies a frequency\nestimation protocol to identify frequent preﬁxes within each\ngroup. Next, we take thePreﬁx Extending Method (PEM)[62],\na state-of-the-art heavy hitter identiﬁcation protocol, as an\nexample to illustrate the process.\nIn PEM, each user encodes its item as a γ-bits binary vec-\ntor. Suppose users are evenly divided into g groups. In the\njth iteration, users in the jth group use the OLH protocol\nto perturb the ﬁrst λj = ⌈log2 k⌉+\n⌈\nj · γ−⌈log2 k⌉\ng\n⌉\nbits of their\nbinary vectors and send the perturbed bits to the central server,\nwhich uses the aggregate step of the OLH protocol to esti-\nmate the frequencies of the preﬁxes that extend the previous\ntop-k preﬁxes. OLH instead of OUE is used because the num-\nber of items corresponding to λj bits is 2λj , which is often\nlarge and incurs large communication costs for OUE. Specif-\nically, the central server uses the aggregate step of OLH to\nestimate the frequencies of the λj-bits preﬁxes in the set\nR j−1 ×{0,1}λj −λj−1 , where R j−1 is the set of top-k λj−1-bits\npreﬁxes identiﬁed in the ( j −1)th iteration and the ×symbol\ndenotes Cartesian product. After estimating the frequencies\nof these λj-bits preﬁxes, the central server identiﬁes the top-k\nmost frequent ones, which are denoted as the set R j. This pro-\ncess is repeated for the g groups and the set of top-k preﬁxes\nin the ﬁnal iteration are identiﬁed as the top-k heavy hitters.\n2.3 Data Poisoning Attacks\nData poisoning attacks to LDP protocols: A concurrent\nwork [13] studied untargeted attacks to LDP protocols. In\nparticular, they focused on degrading the overall performance\nof frequency estimation or heavy hitter identiﬁcation. For\ninstance, we can represent the estimated frequencies of all\nitems as a vector, where an entry corresponds to an item. They\nstudied how an attack can manipulate the Lp-norm distance\nbetween such vectors before and after attack. In contrast,\nwe study targeted attacks that aim to increase the estimated\nfrequencies of the attacker-chosen target items or promote\nthem to be identiﬁed as heavy hitters. We note that the Lp-\nnorm distance between the item frequency vectors is different\nfrom the increased estimated frequencies for the target items.\nFor instance, L1-norm distance between the item frequency\nvectors is a loose upper bound of the increased estimated\nfrequencies for the target items.\nData poisoning attacks to machine learning: A line of\nworks [7, 11, 23–25, 27–30, 35–39, 41–44, 49, 50, 58, 65] stud-\nied data poisoning attacks to machine learning systems. In\nparticular, the attacker manipulates the training data such\nthat a bad model is learnt, which makes predictions as the at-\ntacker desires. For instance, Biggio et al. [11] investigated data\npoisoning attacks against Support Vector Machines. Jagiel-\nski et al. [ 29] studied data poisoning attacks to regression\nmodels. Shafahi et al. [ 50] proposed poisoning attacks to\nneural networks, where the learnt model makes incorrect\npredictions only for target testing examples. Gu et al. [ 27]\nand Liu et al. [ 36] proposed data poisoning attacks (also\ncalled backdoor/trojan attacks) to neural networks, where\nthe learnt model predicts an attacker-chosen label for test-\ning examples with a certain trigger. Data poisoning attacks\nwere also proposed to spam ﬁlters [ 41], recommender sys-\ntems [24,25,35,65], graph-based methods [55], etc.. Our data\npoisoning attacks are different from these attacks because\nhow LDP protocols aggregate the users’ data to estimate fre-\nquencies or identify heavy hitters is substantially different\nfrom how a machine learning system aggregates training data\nto derive a model.\n3 Attacking Frequency Estimation\n3.1 Threat Model\nWe characterize our threat model with respect to an attacker’s\ncapability, background knowledge, and goal.\nAttacker’s capability and background knowledge:We as-\nsume an attacker can inject some fake users into an LDP\nprotocol. These fake users can send arbitrary data in the en-\ncoded space to the central server. Speciﬁcally, we assume\nn genuine users and the attacker injects m fake users to the\nsystem. Therefore, the total number of users becomes n + m.\nWe note that it is a practical threat model to assume that an\n\nattacker can inject fake users.In particular, previous measure-\nment study [54] showed that attackers can easily have access\nto a large number of fake/compromised accounts in various\nweb services such as Twitter, Google, and Hotmail. Moreover,\nan attacker can buy fake/compromised accounts for these\nweb services from merchants in the underground market with\ncheap prices. For instance, a Hotmail account costs $0.004 –\n0.03; and a phone veriﬁed Google account costs $0.03 – 0.50\ndepending on the merchants.\nSince an LDP protocol executes the encode and perturb\nsteps locally on users’ side, the attacker has access to the\nimplementation of these steps. Therefore, the attacker knows\nvarious parameters of the LDP protocol. In particular, the\nattacker knows the domain size d, the encoded space D, and\nthe support set S(y) for each perturbed value y ∈D.\nAttacker’s goal: We consider the attacker’s goal is to pro-\nmote some target items, i.e., increase the estimated frequen-\ncies of the target items. For example, a company may be\ninterested in making its products more popular. Formally,\nwe denote by T = {t1,t2,··· ,tr} the set of r target items. To\nincrease the estimated frequencies of the target items, the at-\ntacker carefully crafts the perturbed values sent from the fake\nusers to the central server. We denote byY the set of crafted\nperturbed values for the fake users, where an entry yi of Y\nis the crafted perturbed value for a fake user. The perturbed\nvalue yi could be a number (e.g., for kRR protocol), a binary\nvector (e.g., for OUE), and a tuple (e.g., for OLH).\nSuppose ˜ft,b and ˜ft,a are the frequencies estimated by the\nLDP protocol for a target item t before and after attack, re-\nspectively. We deﬁne thefrequency gainΔ ˜ft for a target item\nt as Δ ˜ft = ˜ft,a − ˜ft,b,∀t ∈T . A larger frequency gain Δ ˜ft im-\nplies a more successful attack. Note that an LDP protocol\nperturbs the value on each genuine user randomly. Therefore,\nthe frequency gain Δ ˜ft is random for a given set of crafted\nperturbed values Y for the fake users. Thus, we deﬁne the\nattacker’s overall gain G using the sum of the expected fre-\nquency gains for the target items, i.e., G(Y)= ∑t∈T E[Δ ˜ft ],\nwhere Δ ˜ft implicitly depends on Y. Therefore, an attacker’s\ngoal is to craft the perturbed values Y to maximize the over-\nall gain. Formally, the attacker aims to solve the following\noptimization problem:\nmax\nY\nG(Y). (8)\nWe note that, to incorporate the different priorities of the\ntarget items, an attacker could also assign different weights to\nthe expected frequency gains E[Δ ˜ft ] of different target items\nwhen calculating the overall gain. Our attacks are also appli-\ncable to such scenarios. However, for simplicity, we assume\nthe target items have the same priority.\n3.2 Three Attacks\nWe propose three attacks: Random perturbed-value attack\n(RPA), random item attack (RIA), and Maximal gain attack\n(MGA). RPA selects a perturbed value from the encoded space\nof the LDP protocol uniformly at random for each fake user\nand sends it to the server. RPA does not consider any informa-\ntion about the target items. RIA selects a target item from the\nset of target items uniformly at random for each fake user and\nuses the LDP protocol to encode and perturb the item. MGA\ncrafts the perturbed value for each fake user to maximize the\noverall gain G via solving the optimization problem in Equa-\ntion (8). RPA and RIA are two baseline attacks, which are\ndesigned to better demonstrate the effectiveness of MGA.\nRandom perturbed-value attack (RPA): For each fake\nuser, RPA selects a value from the encoded space of the LDP\nprotocol uniformly at random and sends it to the server.\nRandom item attack (RIA):\nUnlike RPA, RIA considers in-\nformation about the target items. In particular, RIA randomly\nselects a target item from the set of target items for each fake\nuser. Then, the LDP protocol is applied to encode and perturb\nthe item. Finally, the perturbed value is sent to the server.\nMaximal gain attack (MGA):The idea behind this attack is\nto craft the perturbed values for the fake users via solving the\noptimization problem in Equation (8). Speciﬁcally, according\nto Equation (3), the frequency gain Δ ˜ft for a target item t is:\nΔ ˜ft =\n1\nn+m\nn+m\n∑\ni=1\n1S(yi )(t) −q\np −q −\n1\nn\nn\n∑\ni=1\n1S(yi )(t) −q\np −q (9)\n=\nn+m\n∑\ni=n+1\n1S(yi )(t)\n(n + m)(p −q) −\nm\nn\n∑\ni=1\n1S(yi )(t)\nn(n + m)(p −q) , (10)\nwhere yi is the perturbed value sent from user i to the server.\nThe ﬁrst term in Equation (10) only depends on fake users,\nwhile the second term only depends on genuine users. More-\nover, the expected frequency gain for a target itemt is:\nE[Δ ˜ft ]=\nn+m\n∑\ni=n+1\nE[1S(yi )(t)]\n(n + m)(p −q) −\nm\nn\n∑\ni=1\nE[1S(yi )(t)]\nn(n + m)(p −q) , (11)\nwhere we denote the second term as a constant ct for simplic-\nity. Moreover, based on Equation (4), we have:\nct = m( ft (p −q)+ q)\n(n + m)(p −q) , (12)\nwhere ft is the true frequency of t among the n genuine users.\nFurthermore, we have the overall gain as follows:\nG =\nn+m\n∑\ni=n+1\n∑\nt∈T\nE[1S(yi )(t)]\n(n + m)(p −q) −c, (13)\nwhere c = ∑t∈T ct = m( fT (p−q)+rq)\n(n+m)(p−q) , where fT = ∑t∈T ft . c\ndoes not depend on the perturbed values sent from the fake\nusers to the central server. In RPA and RIA, the crafted per-\nturbed values for the fake users are random. Therefore, the\nexpectation of the characteristic function E[1S(yi)(t)] and the\noverall gain depend on such randomness. However, MGA\n\nuses the optimal perturbed values for fake users, and the char-\nacteristic function 1S(yi)(t) becomes deterministic. Therefore,\nfor MGA, we can drop the expectationE in Equation (13), and\nthen we can transform the optimization problem in Equation\n(8) as follows:\nY∗ = argmax\nY\nG(Y)= argmax\nY\nn+m\n∑\ni=n+1\n∑\nt∈T\n1S(yi )(t), (14)\nwhere we remove the constantsc and (n +m)(p −q) in the op-\ntimization problem. Note that the above optimization problem\nonly depends on the perturbed values of the fake users, and\nthe perturbed valuesyi for the fake users are independent from\neach other. Therefore, we can solve the optimization prob-\nlem independently for each fake user. Formally, for each fake\nuser, we craft its perturbed value y∗ via solving the following\noptimization problem:\ny∗ = argmax\ny∈D\n∑\nt∈T\n1S(y)(t). (15)\nWe note that, for each fake user, we obtain its perturbed\nvalue via solving the same above optimization problem. How-\never, as we will show in the next sections, the optimization\nproblem has many optimal solutions. Therefore, we randomly\npick an optimal solution for a fake user.\nNext, we discuss how to apply these three attacks to state-\nof-the-art LDP protocols including kRR, OUE, and OLH, as\nwell as analyzing their overall gains.\n3.3 Attacking kRR\nRandom perturbed-value attack (RPA):For each fake user,\nRPA randomly selects a perturbed value yi from the encoded\nspace, i.e., [d], and sends it to the server. We can calculate the\nexpectation of the characteristic function for t ∈T as follows:\nE[1S(yi )(t)] =Pr(1S(yi )(t)= 1) (16)\n= Pr(t ∈S(yi)) =Pr(yi = t) (17)\n= 1\nd (18)\nTherefore, according to Equation ( 13), the overall gain is\nG = rm\nd(n+m)(p−q) −c.\nRandom item attack (RIA): For each fake user, RIA ran-\ndomly selects an item ti from the set of target items T , per-\nturbs the item following the rule in Equation (5), and sends\nthe perturbed item yi to the server. First, we can calculate the\nexpectation of the characteristic function as follows:\nE[1S(yi )(t)] =Pr(yi = t) (19)\n= Pr(ti = t)Pr(yi = t|ti = t)\n+ Pr(ti ̸= t)Pr(yi = t|ti ̸= t) (20)\n= 1\nr ·p +( 1 − 1\nr )q, (21)\nwhere r = |T | is the number of target items. According\nto Equation ( 13), we can obtain the overall gain as G =\n(p+(r−1)q)m\n(n+m)(p−q) −c.\nMaximal gain attack (MGA): For each fake user, MGA\ncrafts its perturbed value by solving the optimization prob-\nlem in Equation ( 15). For the kRR protocol, we have\n∑t∈T 1S(y)(t) ≤ 1 and ∑t∈T 1S(y)(t)= 1 when y is a target\nitem in T . Therefore, MGA picks any target item for each\nfake user. Moreover, according to Equation (13), the overall\ngain is G = m\n(n+m)(p−q) −c.\n3.4 Attacking OUE\nRandom perturbed-value attack (RPA):For each fake user,\nRPA selects a d-bits binary vector yyyi from the encoded space\n{0,1}d uniformly at random as its perturbed vector and sends\nit to the server. We denote by yi, j the j-th bit of the per-\nturbed vector yyyi. Therefore, for each target item t ∈ T ,w e\nhave E[1S(yyyi)(t)] =Pr(yi,t = 1)= 1\n2 . According to Equation\n(13), we can obtain the overall gain as G = rm\n2(n+m)(p−q) −c.\nRandom item attack (RIA): For each fake user, RIA ran-\ndomly selects a target itemti ∈T , encodes it to ad-bits binary\nvector ei whose bits are all zeros except theti-th bit, randomly\nperturbs ei following Equation (6), and sends the perturbed\nvector yyyi to the server. For a target itemt ∈T , we can calculate\nthe expected value of the characteristic function as follows:\nE[1S(yyyi )(t)] =Pr(yi,t = 1) (22)\n= Pr(ti = t)Pr(yi,t = 1|ti = t)\n+ Pr(ti ̸= t)Pr(yi,t = 1|ti ̸= t) (23)\n= 1\nr ·p +( 1 − 1\nr ) ·q, (24)\nwhere p and q are deﬁned in Equation ( 6). Therefore, the\noverall gain is G = (p+(r−1)q)m\n(n+m)(p−q) −c.\nMaximal gain attack (MGA): For each fake user, MGA\nchooses a perturbed vectoryyyi that is a solution of the optimiza-\ntion problem deﬁned in Equation ( 15). For OUE, we have\n∑t∈T 1S(yyyi)(t) ≤ r and ∑t∈T 1S(yyyi)(t)= r is achieved when\n1S(yyyi)(t)= 1,∀t ∈T . Thus, for each fake user, MGA initial-\nizes a perturbed vector yyyi as a binary vector of all 0’s and sets\nyi,t = 1 for all t ∈T . However, if all fake users send the same\nperturbed binary vector to the server, the server can easily\ndetect the fake users. For instance, there is only one entry in\nthe perturbed binary vector that has value 1 when we only\nhave 1 target item; and the server could detect a vector with\nonly a single 1 to be from a fake user, because it is statistically\nunlikely for a genuine user to send such a vector. Therefore,\nMGA also randomly samplesl non-target bits of the perturbed\nvector yyyi and sets them to 1. Speciﬁcally, we set l such that\nthe number of 1’s in the binary vector is the expected number\nof 1’s in the perturbed binary vector of a genuine user. Since\nthe perturbed binary vector of a genuine user hasp +(d −1)q\n1’s on average, we set l = ⌊p +( d −1)q −r⌋. Note that r is\nusually much smaller than d,s o l is a non-negative value. The\nﬁnal binary vector is sent to the server. According to Equation\n(13), the overall gain is G = rm\n(n+m)(p−q) −c.\n\nkRR OUE OLH\nRandom perturbed-value attack (RPA) β( r\nd − fT ) β(r − fT ) −β fT\nRandom item attack (RIA) β(1 − fT ) β(1 − fT ) β(1 − fT )\nMaximal gain attack (MGA) β(1 − fT )+ β(d−r)\neε−1 β(2r − fT )+ 2βr\neε−1 β(2r − fT )+ 2βr\neε−1\nStandard deviation of estimation r\n√\nd−2+eε\n(eε−1)√n\n2reε/2\n(eε−1)√n\n2reε/2\n(eε−1)√n\nTable 1: Overall gains of the three attacks for kRR, OUE, and OLH.n is the number of genuine users,β = m\nn+m is the\nfraction of fake users among all users,d is the number of items,r is the number of target items,fT = ∑t∈T ft is the sum\nof true frequencies of the target items among the genuine users,ε is the privacy budget, ande is the base of the natural\nlogarithm. To understand the signiﬁcance of the overall gains, we also include the standard deviations of the estimated\ntotal frequencies of the target items among then genuine users [59] in the table.\n3.5 Attacking OLH\nRandom perturbed-value attack (RPA):For each fake user,\nRPA randomly selects a hash functionHi ∈H and a hash value\nai ∈[d′], and sends the tuple yi =( Hi,ai) to the server. For\neach t ∈T ,w eh a v eE[1S(yyyi)(t)] =Pr(Hi(t)= ai)= 1\nd′. There-\nfore, we can obtain the overall gain as G = rm\nd′(n+m)(p−q) −c.\nRandom item attack (RIA): For each fake user, RIA ran-\ndomly selects a target itemti, randomly selects a hash function\nHi ∈H, and calculates the hash value hi = Hi(ti). The tuple\n(Hi,hi) is then perturbed as (Hi,ai) according to Equation (7).\n(Hi,ai) is the perturbed value, i.e., yi =( Hi,ai). We assume\nthe hash function Hi maps any item in [d] to a value in [d′]\nuniformly at random. For a target itemt ∈T , we can calculate\nthe expectation of the characteristic function as follows:\nE[1S(yi )(t)] =Pr(Hi(t)= ai) (25)\n= Pr(ti = t)Pr(Hi(t)= ai|ti = t)\n+ Pr(ti ̸= t)Pr(Hi(t)= ai|ti ̸= t) (26)\n= 1\nr ·p +( 1 − 1\nr ) ·q. (27)\nThus, the overall gain for RIA is G = [p+(r−1)q]m\n(n+m)(p−q) −c.\nMaximal gain attack (MGA): For each fake user, MGA\nchooses a perturbed value yi =( Hi,ai) that is a solution of\nthe optimization problem deﬁned in Equation (15). For OLH,\nwe have ∑t∈T 1S(yi)(t) ≤r and ∑t∈T 1S(yi)(t)= r is achieved\nwhen the hash function Hi maps all items in T to ai, i.e.,\nHi(t)= ai,∀t ∈T . Thus, for each fake user, MGA searches\nfor a hash function Hi in H such that Hi(t)= ai,∀t ∈T holds.\nTherefore, according to Equation ( 13), the overall gain is\nG = rm\n(n+m)(p−q) −c. Note that we may not be able to ﬁnd such\na hash function in practice. In our experiments, for each fake\nuser, we randomly sample 1,000 hash functions and use the\none that hashes the most target items to the same value.\n3.6 Theoretical Analysis\nTable 1 summarizes the overall gains of the three attacks for\nkRR, OUE, and OLH, where we have replaced the parameters\np and q for each LDP protocol according to Section2.1. Next,\nwe compare the three attacks, discuss a fundamental security-\nprivacy tradeoff, and compare the three LDP protocols with\nrespect to their security against our data poisoning attacks.\nComparing the three attacks: All three attacks achieve\nlarger overall gains when the target items’ true frequencies\nare smaller (i.e.,\nfT is smaller). MGA achieves the largest\noverall gain among the three attacks. In fact, given an LDP\nprotocol, a set of target items and fake users, MGA achieves\nthe largest overall gain among all possible attacks. This is\nbecause MGA crafts the perturbed values for the fake users\nsuch that the overall gain is maximized. RIA achieves larger\noverall gains than RPA for kRR and OLH, while RPA achieves\na larger overall gain than RIA for OUE.\nTable 1 also includes the standard deviations of the es-\ntimated total frequencies of the target items among the n\ngenuine users. Due to the √n term in the denominators, the\nstandard deviations are much smaller than the overall gains\nof our MGA attacks. For instance, on the Zipf dataset in our\nexperiments with the default parameter settings, the overall\ngains of MGA are 1600, 82, and 82 times larger than the\nstandard deviations for kRR, OUE, and OLH, respectively.\nFundamental security-privacy tradeoffs: The security of\nan LDP protocol is determined by the strongest attack (i.e.,\nMGA) to it. Intuitively, when the privacy budgetε is smaller\n(i.e., stronger privacy), genuine users add larger noise to their\ndata. However, the perturbed values that MGA crafts for the\nfake users do not depend on the privacy budget. As a result,\nthe fake users contribute more towards the estimated item\nfrequencies, making the overall gain larger. In other words,\nwe have a fundamental security-privacy tradeoff. Formally,\nthe following theorem shows such tradeoffs.\nTheorem 1(Security-Privacy Tradeoff). F or any of the three\nLDP protocols kRR, OUE, and OLH, when the privacy budget\nε is smaller (i.e., stronger privacy), MGA achieves a larger\noverall gain G (i.e., weaker security).\nProof. Table 1 shows that ε is in the denominator of the\noverall gains for MGA. Therefore, the overall gains of MGA\nincrease as ε decreases.\n\nComparing the security of the three LDP protocols:Ta-\nble 1 shows that, when MGA is used, OUE and OLH achieve\nthe same overall gain. Therefore, OUE and OLH have the\nsame level of security against data poisoning attacks. The\nfollowing theorem shows that OUE and OLH are more secure\nthan kRR when the number of items is larger than a threshold.\nTheorem 2. Suppose MGA is used. OUE and OLH are more\nsecure than kRR when the number of items is larger than some\nthreshold, i.e., d> (2r −1)(eε −1)+ 3r.\nProof. See Appendix A.\n4 Attacking Heavy Hitter Identiﬁcation\n4.1 Threat model\nAttacker’s capability and background knowledge: We\nmake the same assumption on the attacker’s capability and\nbackground knowledge as in attacking frequency estimation,\ni.e., the attacker can inject fake users into the protocol and\nsend arbitrary data to the central server.\nAttacker’s goal: We consider the attacker’s goal is to pro-\nmote some target items, i.e., manipulate the heavy hitter iden-\ntiﬁcation protocol to recognize the target items as top-k heavy\nhitters. Formally, we denote byT = {t1,t2,··· ,tr}the set of r\ntarget items, which are not among the true top-k heavy hitters.\nWe deﬁne success rate of an attack as the fraction of target\nitems that are promoted to be top-k heavy hitters by the attack.\nAn attacker’s goal is to achieve a high success rate.\n4.2 Attacks\nState-of-the-art heavy hitter identiﬁcation protocols iteratively\napply frequency estimation protocols. Therefore, we apply\nthe three attacks for frequency estimation to heavy hitter iden-\ntiﬁcation. Next, we use PEM as an example to illustrate how\nto attack heavy hitter identiﬁcation protocols.\nIn PEM, each item is encoded by a γ-bits binary vector\nand users are randomly divided into g groups. On average,\neach group contains a fraction of m\nn+m fake users. In the jth\niteration, PEM uses OLH to perturb the ﬁrst λj bits of the\nbinary vectors for users in the jth group and sends them to\nthe central server. An attacker uses the RPA, RIA, or MGA to\ncraft the data sent from the fake users to the central server by\ntreating the ﬁrst λj bits of the binary vectors corresponding\nto the target items as the “target items” in the jth iteration.\nSuch attacks can increase the likelihood that the ﬁrst λj bits\nof the target items are identiﬁed as the top-k preﬁxes in the\njth iteration, which in turn makes it more likely to promote\nthe target items as top-k heavy hitters.\nParameter Default setting\nβ 0.05\nr 1\nfT 0.01\nε 1\nk 20\ng 10\nTable 2: Default parameter settings.\n5 Evaluation\n5.1 Experimental Setup\nDatasets: We evaluate our attacks on three datasets, in-\ncluding a synthetic dataset and two real-world datasets, i.e.,\nFire [4] and IPUMS [51].\n• Zipf: Following previous work on LDP protocols, we\ngenerate random data following the Zipf’s distribution. In\nparticular, we use the same parameter in the Zipf’s distri-\nbution as in [59]. By default, we synthesize a dataset with\n1,024 items and 1,000,000 users.\n• Fire [4]: The Fire dataset was collected by the San Fran-\ncisco Fire Department, recording information about calls\nfor service. We ﬁlter the records by call type and use the\ndata of type “Alarms”. We treat the unit ID as the item that\neach user holds, which results in a total of 244 items and\n548,868 users.\n• IPUMS [51]: The IPUMS dataset contains the US census\ndata over the years. We select the latest data of 2017 and\ntreat the city attribute as the item each user holds, which\nresults in a total of 102 items and 389,894 users.\nParameter setting: For frequency estimation, the overall\ngains of our attacks may depend on β (the fraction of fake\nusers), r and fT (the number of target items and their true\nfrequencies), ε (privacy budget), and d (number of items in\nthe domain). For heavy hitter identiﬁcation, the success rates\nof our attacks further depend on k (the number of items iden-\ntiﬁed as heavy hitters) and g (the group size used by the\nPEM protocol). Table 2 shows the default settings for these\nparameters, which we will use in our experiments unless other-\nwise mentioned. We will study the impact of each parameter,\nwhile ﬁxing the remaining parameters to their default settings.\nMoreover, we use d′= ⌈eε + 1⌉in OLH as d′is an integer.\n5.2 Results for Frequency Estimation\nImpact of different parameters:Table 1 shows the theoret-\nical overall gains of the three attacks for the kRR, OUE, and\nOLH protocols. We use these theoretical results to study the\nimpact of each parameter. Figures 1 to 3 show the impact of\ndifferent parameters on the overall gains andnormalized over-\nall gains. A normalized overall gain is the ratio between the\ntotal frequencies of the target items after and before an attack,\n\nFigure 1: Impact of different parameters on the overall gains (ﬁrst row) and normalized overall gains (second row) of\nthe three attacks for kRR.\nFigure 2: Impact of different parameters on the overall gains (ﬁrst row) and normalized overall gains (second row) of\nthe three attacks for OUE.\nFigure 3: Impact of different parameters on the overall gains (ﬁrst row) and normalized overall gains (second row) of\nthe three attacks for OLH.\ni.e., (G + fT )/ fT , where fT is the total true frequencies of the\ntarget items. We observe that MGA outperforms RIA, which\noutperforms RPA or achieves similar (normalized) overall\ngains with RPA. The reason is that MGA is an optimization-\nbased attack, RIA considers information of the target items,\nand RPA does not consider information about the target items.\nNext, we focus our analysis on MGA since it is the strongest\nattack. The (normalized) overall gains of MGA increase as\nthe attacker injects more fake users, the attacker promotes\nmore target items (except the kRR protocol), or the privacy\nbudget\nε becomes smaller (i.e., security-privacy tradeoffs).\nThe (normalized) overall gain of MGA decreases as the total\ntrue frequency of the target items (i.e., fT ) increases, though\nthe decrease of the overall gain is marginal. The (normalized)\noverall gain of MGA increases for kRR but keeps unchanged\nfor OUE and OLH as d increases. We note that, for a given\n\nFigure 4: Impact of different parameters on the success rates of the three attacks for PEM (heavy hitter identiﬁcation\nprotocol). The ﬁrst row is on Zipf, the second row is on Fire, and the third row is on IPUMS.\nset of target items (i.e., fT is given), the trend of normalized\noverall gain is the same as that of the overall gain with respect\nto parameters\nβ, r, ε, and d. Therefore, in the rest of the paper,\nwe focus on overall gain for simplicity.\nMeasuring RIA and MGA for OLH:The theoretical over-\nall gain of RIA for OLH is derived based on the “perfect”\nhashing assumption, i.e., an item is hashed to a value in the\nhash domain [d′] uniformly at random. Practical hash func-\ntions may not satisfy this assumption. Therefore, the theoreti-\ncal overall gain of RIA for OLH may be inaccurate in practice.\nWe use xxhash [14] as hash functions to evaluate the gaps be-\ntween the theoretical and practical overall gains. In particular,\nFigure 5a compares the theoretical and practical overall gains\nof RIA for OLH, where 1 item is randomly selected as target\nitem, β = 0.05, and ε = 1. We observe that the theoretical and\npractical overall gains of RIA for OLH are similar.\nOur theoretical overall gain of MGA for OLH is derived\nbased on the assumption that the attacker can ﬁnd a hash\nfunction that hashes all target items to the same value. In\npractice, we may not be able to ﬁnd such hash functions\nwithin a given amount of time. Therefore, for each fake user,\nwe randomly sample some xxhash hash functions and use\nthe one that hashes the most target items to the same value.\nFigure 5b compares the theoretical and practical overall gains\nof MGA for OLH on the IPUMS dataset as we sample more\nhash functions for each fake user, where we randomly select\n5 items as target items, i.e., r = 5. Our results show that the\npractical overall gains approach the theoretical ones with\nseveral hundreds of randomly sampled hash functions when\nr = 5. We have similar observations for the other two datasets\nand thus we omit their results due to the limited space.\n(a) (b)\nFigure 5: (a) Theoretical and practical overall gains of\nRIA for OLH. (b) Theoretical and practical overall gains\nof MGA for OLH on the IPUMS dataset as we sample\nmore hash functions for each fake user, wherer = 5.\n5.3 Results for Heavy Hitter Identiﬁcation\nFigure 4 shows the empirical results of applying our three\nattacks, i.e., RPA, RIA and MGA, to PEM on the Zipf, Fire,\nand IPUMS datasets, respectively. By default, we randomly\nselect r = 10 target items that are not identiﬁed as top-k heavy\nhitters by PEM before attack and use the three attacks to\npromote them. Default values for the other parameters are\nidentical to those in Table 2. The success rate of an attack\nis calculated as the fraction of target items that appear in\nthe estimated top-\nk heavy hitters. The results show that our\nMGA attacks can effectively compromise the PEM protocol.\nIn particular, we observe that MGA only needs about 5% of\nfake users to achieve a 100% success rate when r = 10 and\nk = 20. In fact, with only 5% of fake users, we can promote\n10 target items to be in the top-15 heavy hitters, or promote 15\ntarget items to be in the top-20 heavy hitters. However, RPA\nand RIA are ineffective. Speciﬁcally, even if we inject 10%\n\nof fake users, neither RPA nor RIA can successfully promote\neven one of the target items to be in the top-k heavy hitters.\nMoreover, the number of groups g and the privacy budget ε\nhave negligible impact on the effectiveness of our attacks.\n6 Countermeasures\nWe explore three countermeasures. The ﬁrst countermeasure\nis to normalize the estimated item frequencies to be a prob-\nability distribution, the second countermeasure is to detect\nfake users via frequent itemset miningof the users’ perturbed\nvalues and remove the detected fake users before estimating\nitem frequencies, and the third countermeasure is to detect the\ntarget item without detecting the fake users when there is only\none target item. The three countermeasures are effective in\nsome scenarios. However, our MGA is still effective in other\nscenarios, highlighting the needs for new defenses against our\ndata poisoning attacks.\n6.1 Normalization\nThe LDP protocols estimate item frequencies using Equation\n(3). Therefore, the estimated item frequencies may not form a\nprobability distribution, i.e., some estimated item frequencies\nmay be negative and they may not sum to 1. For instance, our\nexperimental results in Section 5.2 show that the overall gains\nof MGA may be even larger than 1. Therefore, one natural\ncountermeasure is to normalize the estimated item frequencies\nsuch that each estimated item frequency is non-negative and\nthe estimated item frequencies sum to 1. For instance, one\nnormalization we consider is as follows: the central server\nﬁrst estimates the frequency\n˜fv for each item v following a\nLDP protocol (kRR, OUE, or OLH); then the server ﬁnds\nthe minimal estimated item frequency ˜fmin; ﬁnally, the server\ncalibrates the estimated frequency for each item v as ¯fv =\n˜fv− ˜fmin\n∑v( ˜fv− ˜fmin) , where ¯fv is the calibrated frequency. Our overall\ngain is calculated by the difference between the calibrated\nfrequencies of the target items after and before attack. We note\nthat there are also other methods to normalize the estimated\nitem frequencies [ 31, 63], which we leave as future work.\nNote that the normalization countermeasure is not applicable\nto heavy hitter identiﬁcation because normalization does not\nimpact the ranking of items’ frequencies.\n6.2 Detecting Fake Users\nRPA and MGA directly craft the perturbed values for fake\nusers, instead of using the LDP protocol to generate the per-\nturbed values from certain items. Therefore, the perturbed\nvalues for the fake users may be statistically abnormal. We\nnote that it is challenging to detect fake users via statistical\nanalysis of the perturbed values for the kRR protocol, because\nthe perturbed value of a user is just an item, no matter whether\nUser 1:\nUser 2:\nUser 3:\nUser 4:\nFigure 6: An example itemset that are all 1’s in 3 of the 4\nbinary vectors. Each column corresponds to an item.\nor not the attacker follows the protocol to generate the per-\nturbed value. Therefore, we study detecting fake users in the\nRPA and MGA attacks for the OUE and OLH protocols. Since\nPEM iteratively applies OLH, we can also apply detecting\nfake users to PEM.\nOUE: Recall that MGA assigns 1 to all target items and l\nrandomly selected items in the perturbed binary vector for\neach fake user. Therefore, among the perturbed binary vectors\nfrom the fake users, a set of items will always be 1. However,\nif the perturbed binary vectors follow the OUE protocol, it\nis unlikely to observe that this set of items are all 1’s for a\nlarge number of users. Therefore, our idea to detect fake users\nconsists of two steps. In the ﬁrst step, the server identiﬁes\nitemsets that are all 1’s in the perturbed binary vectors of a\nlarge number of users. In the second step, the server detects\nfake users if the probability that such large number of users\nhave these itemsets of all 1’s is small, when following OUE.\nStep I. In this step, the server identiﬁes itemsets that are\nfrequently all 1’s among the perturbed binary vectors. Figure6\nshows an example itemset that are all 1’s in 3 of the 4 binary\nvectors. Identifying such itemsets is also known as frequent\nitemset mining[6]. In our problem, given the perturbed binary\nvectors from all users, frequent itemset mining can ﬁnd the\nitemsets that are all 1’s in at least a certain number of users.\nSpeciﬁcally, a frequent itemset mining method produces some\ntuples BBB = {(B,s)|s ≥τ}, where B is an itemset and s is the\nnumber of users whose perturbed binary vectors are 1’s for\nall items in B.\nStep II. In this step, we determine whether there are fre-\nquent itemsets that are statistically abnormal. Speciﬁcally,\nwe predict a tuple (B,s) ∈BBB to be abnormal if s ≥τz, where\nz = |B| is the size of the itemset B. When an itemset is pre-\ndicted to be abnormal, we predict the items as the target items\nand the users whose perturbed binary vectors are 1’s for all\nitems in the itemset to be fake. The threshold τz achieves a\ntradeoff between false positive rateand false negative rateof\ndetecting fake users. Speciﬁcally, when τz is larger, a smaller\nnumber of genuine users are predicted as fake (i.e., a smaller\nfalse positive rate), while a larger number of fake users are\nnot detected (i.e., a larger false negative rate). Therefore, a\nkey challenge is how to select the threshold τz. We propose\nto select the threshold such that the false positive rate is at\nmost\nη. Speciﬁcally, given a threshold τz > (n + m)pqz−1,\n\nwe can derive an upper bound of the false positive rate as\n(n+m)pqz−1(1−pqz−1)\n[τz−(n+m)pqz−1]2 (see Appendix B for details). Therefore,\nto guarantee that the false positive rate is at mostη and achieve\na small false negative rate, we select the smallest τz that sat-\nisﬁes τz > (n + m)pqz−1 and (n+m)pqz−1(1−pqz−1)\n[τz−(n+m)pqz−1]2 ≤η.W es e t\nη = 0.01 in our experiments.\nOLH: To attack the OLH protocol, MGA searches a hash\nfunction for each fake user that hashes as many target items\nto the same value as possible. Suppose we construct a d-\nbit binary vector yyy for each user with a tuple (H,a) such\nthat yv = 1 if and only if H(v)= a. Then, the target items\nwill be 1’s in the binary vectors for a large number of users.\nTherefore, we can also leverage the method to detect fake\nusers in OLH. Speciﬁcally, in Step I, we ﬁnd frequent item-\nsets in the constructed binary vectors. In Step II, we predict\nan itemset B to be abnormal if its number of occurrences s\namong the n + m binary vectors is larger than a threshold\nτz, where z = |B| is the size of the itemset. Like OUE, we\nselect the threshold τz such that the false positive rate is at\nmost η. Speciﬁcally, we select the smallest τz that satisﬁes\nI(qz−1;τz,n + m −τz + 1) ≤η, where I is the regularized in-\ncomplete beta function[5]. I(qz−1;τz,n + m −τz + 1) is the\nfalse positive rate for a given τz (see Appendix B for details).\nPEM: The heavy hitter identiﬁcation protocol PEM itera-\ntively applies OLH to identify heavy hitters. Therefore, we\ncan apply the frequent itemset mining based detection method\nto detect fake users in PEM. Speciﬁcally, in each iteration of\nPEM, the central server applies the detection method in OLH\nto detect fake users in PEM; and the central server removes\nthe predicted fake users before computing the top-k preﬁxes.\n6.3 Conditional Probability based Detection\nThe frequent itemset mining based detection method above\nrequires at least two target items as it identiﬁes the abnor-\nmal frequent itemset as the target items. When there is only\none target item, i.e., r = 1, it fails to detect the target item.\nTherefore, we discuss another method to detect the target item\nwhen r = 1, which leverages conditional probabilities. Note\nthat this method does not detect fake users.\nOUE: Suppose yyy is a user’s perturbed binary vector. With a\nlittle abuse of notation, we denote the j-th bit of yyy as y j.G i v e n\nthe target item t and a random item j, we have the following\nequations under our MGA attacks to OUE:\nPr(y j = yt = 1)= Pr(v = t) ·Pr(y j = yt = 1|v = t)\n+ Pr(v = j) ·Pr(y j = yt = 1|v = j)\n+ Pr(v ̸= t, j) ·Pr(y j = yt = 1|v ̸= t, j)\n+ Pr(fake) ·Pr(y j = yt = 1|fake) (28)\n= nf t\nn + m ·pq + nf j\nn + m ·pq\n+ n(1 − ft − f j )\nn + m ·q2 + m\nn + m · l\nd −1 , (29)\nf j 0.01 0.01 0.1 0.1 0.5 0.5 0.9 0.9\nft 0 0.01 0 0.01 0 0.01 0 0.01\nˆfu 0.25 0.26 0.18 0.19 0.18 0.18 0.18 0.19\n(a) β = 0.05\nf j 0.01 0.01 0.1 0.1 0.5 0.5 0.9 0.9\nft 0 0.01 0 0.01 0 0.01 0 0.01\nˆfu 1.8 1.8 0.87 0.88 0.82 0.84 0.82 0.83\n(b) β = 0.2\nTable 3: Thresholdˆfu for different f j and ft .\nPr(yt = 1)= Pr(v = t) ·Pr(yt = 1|v = t)\n+ Pr(v ̸= t) ·Pr(yt = 1|v ̸= t)\n+ Pr(fake) ·Pr(yt = 1|fake) (30)\n= nf t\nn + m ·p + n(1 − ft )\nn + m ·q + m\nn + m , (31)\nPr(y j = 1|yt = 1)= Pr(y j = yt = 1)\nPr(yt = 1) (32)\n=q +\nf j q(p −q)+ β\n1−β ·( l\nd−1 −q)\nft p +( 1 − ft )q + β\n1−β\n. (33)\nGiven a non-target item u ̸= j, we have the following:\nPr(y j = yu = 1)\n= Pr(v = u) ·Pr(y j = yu = 1|v = u)\n+ Pr(v = j) ·Pr(y j = yu = 1|v = j)\n+ Pr(v ̸= j,u) ·Pr(y j = yu = 1|v ̸= j,u)\n+ Pr(fake) ·Pr(y j = yu = 1|fake) (34)\n= nf u\nn + m ·pq + nf j\nn + m ·pq + n(1 − fu − f j )\nn + m ·q2\n+ m\nn + m · l\nd −1 · l −1\nd −2 , (35)\nPr(yu = 1)\n= Pr(v = u) ·Pr(yu = 1|v = u)\n+ Pr(v ̸= u) ·Pr(yu = 1|v ̸= u)\n+ Pr(fake) ·Pr(yu = 1|fake) (36)\n= nf u\nn + m ·p + n(1 − fu)\nn + m ·q + m\nn + m · l\nd −1 , (37)\nPr(y j = 1|yu = 1)\n= Pr(y j = yu = 1)\nPr(yu = 1) (38)\n= q +\nf j q(p −q)+ β\n1−β · l\nd−1 ·( l−1\nd−2 −q)\nfu p +( 1 − fu)q + β\n1−β · l\nd−1\n. (39)\nSuppose both t and u are among the top-N items with the\nlargest estimated frequencies. The true frequency ft for the\ntarget item t is small, since our attack aims to promote an\nunpopular item. We havePr(y j = 1|yt = 1) < Pr(y j = 1|yu =\n1) when fu is smaller than a threshold ˆfu. Table 3 shows such\nthreshold for different values of f j and ft , where β = 0.05\nand β = 0.2. We observe that fu is highly likely smaller than\n\nkRR OUE OLH\nNo Norm No Norm Detect Both No Norm Detect Both\nRPA 2e-3 -1e-3 0.50 2e-3 0.50 2e-3 -2e-3 -2e-3 -2e-3 -2e-3\nRIA 0.05 -4e-3 0.05 0.03 – – 0.05 0.03 – –\nMGA 2.72 0.43 1.58 0.46 7e-17 -2e-16 1.18 0.43 1.18 0.43\nTable 4: Overall gains of the three attacks on the IPUMS\ndataset after countermeasures are deployed. The column\n“No” means no countermeasure is used. The column\n“Both” means the combined countermeasure. “–” means\nthat the countermeasure is not applicable. Only normal-\nization is applicable for kRR.\nthe threshold ˆfu for a variety of f j when β = 0.2,a s ˆfu is\nvery large (sometimes even larger than 1). This observation\nshows that if we randomly pick an item as j and compare\nthe conditional probabilities Pr(y j = 1|yu = 1) for each item\nu in the top- N items, then we can detect the item with the\nsmallest conditional probability as the target item. However,\nwhen β = 0.05, the effectiveness of such detection method\ndepends on the true frequencies f j and fu.\nOLH: The conditional probability based detection method\ncan also be used for OLH when r = 1. Speciﬁcally, we can\nconstruct a d-bit binary vector yyy for each user whose vth entry\nyv = 1 if and only if H(v)= a, where (H,a) is the user’s\nperturbed value. Assuming the hash function hashes an item\nuniformly at random to a hash value in [d′]. Then, we have\nthe following conditional probabilities:\nPr(y j = 1|yt = 1)= q + f j q(p −q)\nft p +( 1 − ft )q + β\n1−β\n, (40)\nPr(y j = 1|yu = 1)= q + f j q(p −q)\nfu p +( 1 − fu)q + β\n1−β ·q\n. (41)\n6.4 Experimental Results\nWe empirically evaluate the effectiveness of the three coun-\ntermeasures. Unless otherwise mentioned, we focus on nor-\nmalization and detecting fake users as the conditional proba-\nbility based detection is only applicable for one target item.\nNote that normalization and detecting fake users can also be\nused together. Speciﬁcally, the central server can ﬁrst detect\nand remove the fake users, and then perform normalization.\nTherefore, we will also evaluate the combined countermea-\nsure. We use the same default experimental setup as those\nin Section 5.1. Moreover, we use the FP-growth algorithm\nimplemented in the Python package mlxtend [46] to identify\nfrequent itemsets.\n6.4.1 Frequency Estimation\nOverall results: Table 4 shows the experimental results with\nno countermeasure, normalization, detection, and combined\ncountermeasure, where β = 0.05 and r = 10. We observe that\nthe countermeasures are effective in some scenarios. For ex-\nample, for OUE, combining the two countermeasures leads to\nan overall gain of -2e-16 for MGA, which means that the esti-\nmated total frequency of the target items is even smaller than\nthe one before attack. However, the countermeasures are inef-\nfective in other scenarios. For instance, MGA can still achieve\na large overall gain of 0.43 for OLH even if both countermea-\nsures are used. Normalization can reduce the overall gains\nof all the three attacks for the three protocols except RPA\nfor OLH. However, MGA still achieves large overall gains\nafter normalization. Detecting fake users is ineffective for\nRPA because RPA randomly samples perturbed values in the\nencoded space for the fake users and thus the perturbed values\ndo not have meaningful statistical patterns. When the counter-\nmeasures are used, MGA is still the most effective attack in\nmost cases. Therefore, we focus on MGA and further study\nthe impact of β and r on the countermeasure effectiveness.\nImpact ofβ and r on MGA: Figure 7a-7b show the impact\nof β on the countermeasures against MGA when we ﬁxr = 10,\nwhile Figure 7c-7d show the results for r when we ﬁx β =\n0.05 on the IPUMS dataset. First, we observe that for OUE,\ndetecting fake users and the combined countermeasure can\neffectively defend against the MGA attacks (i.e., reduce the\noverall gains to almost 0) when β and r are larger than some\nthresholds, e.g., β > 0.001 and r ≥3. The countermeasures\nare ineffective when β or r is small (e.g., β ≤0.001 or r ≤2).\nThis is because the detection method relies on that the target\nitemset is frequent and abnormal, but the target itemset is not\nfrequent when β is small and is not abnormal among the users’\nperturbed values when r is small.\nSecond, for OLH, detecting fake users and the combined\ncountermeasure can effectively defend against the MGA at-\ntacks only when r is not too small nor large, e.g., 3 ≤r ≤5\nin our experiments. Recall that, to attack OLH, our MGA\nrandomly samples 1,000 hash functions and uses the one that\nhashes the largest number of target items to the same value\nfor each fake user. When r ≤ 5, our MGA can ﬁnd a hash\nfunction that hashes all target items to the same value. There-\nfore, the target itemset is frequent among the users’ perturbed\nvalues. Moreover, when r ≥3, the frequent target itemset is\nalso abnormal. As a result, the detection method can detect\nMGA when 3 ≤r ≤5. When r ≥6, our MGA can only ﬁnd\na hash function among the 1,000 random ones that hashes a\nsubset of the target items to the same value for each fake user.\nIn other words, each fake user essentially randomly picks a\nsubset of the target items and promotes them. Therefore, the\nentire target itemset is not frequent enough and MGA evades\ndetection. Our MGA evades detection for all the explored β\nin Figure 7b because r = 10 in these experiments.\nAdaptive MGA to OUE: Inspired by the evasiveness of\nMGA to OLH, we can also adapt MGA to OUE that evades\ndetection. Speciﬁcally, for each fake user, instead of using a\nperturbed value that supports all r target items, we randomly\nselect r′of the r target items and ﬁnd a perturbed value that\n\n(a) OUE (b) OLH (c) OUE (d) OLH (e) Adaptive MGA\nFigure 7: (a)-(b) Impact ofβ on the countermeasures against MGA whenr = 10. (c)-(d) Impact ofr on the countermea-\nsures against MGA whenβ = 0.05. (e) Impact ofr′on the adaptive MGA (MGA-A) to OUE whenr = 10.\n(a) N (b) β\nFigure 8: Impact ofN and β on the detection rate of the\nconditional probability based method forr = 1.\nsupports the r′selected target items. The adaptive attack splits\nthe frequency of the target itemset with size r to\n(r\nr′\n)\nitemsets\nwith size r′, which becomes much harder to detect. We call\nsuch adaptive attacks MGA-A. Figure 7e shows the impact\nof r′on MGA-A to OUE when r = 10. We observe that our\nadaptive MGA achieves smaller overall gains asr′becomes\nsmaller when no countermeasures are deployed. However,\nour adaptive MGA evades detection when r′< r.\nAttack stealthiness: If the frequent itemset mining based\ndetection method returns an abnormal frequent itemset, then\nthe central server predicts that it is under our MGA attack.\nOur attack is stealthy if the central server cannot detect it. Our\nresults show that, for OUE, our MGA is stealthy when β or r\nis small (e.g., β ≤0.001 or r ≤2), and our adaptive MGA is\nstealthy when r′< r. For OLH, our MGA is stealthy whenr is\nsmall or large enough, e.g.,r ≤2 or r ≥6 in our experiments.\nConditional probability based detection for r = 1: We\nmeasure the effectiveness of the conditional probability based\ndetection method using detection rate. Speciﬁcally, in each\nexperiment, we perform our MGA attack with a random target\nitem 50 times and the detection rate is the fraction of the 50\nexperiment trials in which the target item is correctly detected.\nFigure 8a shows the impact of N on the detection rate when\nwe ﬁx β = 0.05 on the IPUMS dataset. We observe that the\ndetection rate ﬁrst increases and then decreases as N grows.\nThis is because when N is too small, e.g., N = 1, the target\nitem is likely not in the top-N items; and when N is too large,\nit’s more likely that there exists a non-target item in the top-N\nitems that has a smaller conditional probability than the target\nitem. We notice that the detection rate is lower for OLH than\nfor OUE. This is because the threshold ˆfu for OLH is smaller\nthan that for OUE, e.g., ˆfu = 0.18 for OLH and ˆfu = 0.26 for\nOUE when ft = f j = 0.01. Figure 8b shows the impact of β\non the detection rate, where we explore N = 1 to 20 to ﬁnd\nthe N that achieves the highest detection rate for each given\nβ. We observe that the detection rate increases as β increases,\nwhich implies that the MGA attack with r = 1 is easier to\ndetect when there are more fake users. Once the target item\nis detected, the server can compute the sum of the estimated\nfrequencies of all non-target items as ˜fU = ∑u̸=t ˜fu and set the\nestimated frequency of the target item as ˜ft = 1 − ˜fU , which\ncan reduce the overall gain of MGA. For instance, the overall\ngain decreases from 2.37 to 0.095 for OLH when β = 0.1.\n6.4.2 Heavy Hitter Identiﬁcation\nNormalization is ineffective for heavy hitter identiﬁcation\nbecause normalization does not impact the ranking of the\nitems’ estimated frequencies. Moreover, the conditional prob-\nability based detection is only applicable to one target item.\nTherefore, we perform experiments on detecting fake users\nfor heavy hitter identiﬁcation. Moreover, we focus on MGA\nbecause RIA and RPA are ineffective even without detecting\nfake users (see Figures 4). We observe that detecting fake\nusers is effective in some scenarios but not in others. For\ninstance, when\nr = 5, detecting fake users can reduce the\nsuccess rate of MGA from 1 to 0, as all fake users can be\ndetected. However, when r = 10, our MGA can still achieve\na success rate of 1.\n6.5 Other Countermeasures\nDetecting fake users is related to Sybil detection in dis-\ntributed systems and social networks. Many methods have\nbeen proposed to mitigate Sybil attacks. For instance, meth-\nods [12, 16, 26, 52, 56, 57, 67, 68] that leverage content, be-\nhavior, and social graphs are developed to detect fake users\nin social networks. Our detection method can be viewed as\na content-based method. Speciﬁcally, our detection method\nanalyzes the statistical patterns of the user-generated content\n(i.e., perturbed values sent to the central server) to detect fake\nusers. However, our detection method is different from the\ncontent-based methods to detect fake users in social networks,\nas the user-generated content and their statistical patterns dif-\n\nfer. Social-graph-based methods are inapplicable when the\nsocial graphs are not available.\nAnother countermeasure is to leverage Proof-of-Work [20],\nlike how Sybil is mitigated in Bitcoin. In particular, before\na user can participate in the LDP protocol, the central server\nsends a random string to the user; and the user is allowed to\nparticipate the LDP protocol after the user ﬁnds a string such\nthat the cryptographic hash value of the concatenated string\nhas a certain property, e.g., the ﬁrst 32 bits are all 0. However,\nsuch method incurs a large computational cost for genuine\nusers, which impacts user experience. Moreover, when users\nuse mobile devices such as phones and IoT devices, it is chal-\nlenging for them to perform the Proof-of-Work. Malicious-\nparty-resistant SMPC could also be used to limit the impact\nof fake users (e.g., [40]). However, such methods generally\nsacriﬁce computational efﬁciency.\n7 Discussion\nApplicability to shufﬂing-based and SMPC-based proto-\ncols: Shufﬂing-based protocols [21] apply shufﬂing to the\nusers’ perturbed vectors such that a better DP guarantee can\nbe derived. Since they still encode and perturb each user’s\ndata, our attacks are applicable. When SMPC-based proto-\ncols have local encoding and perturbation steps like [34], our\nattacks are applicable and the security-privacy trade-off still\nholds. When there is no local encoding or perturbation step in\nthe SMPC-based DP protocols like [48], our RPA and MGA\nare not applicable because an attacker cannot manipulate the\nperturbed vectors. However, our RIA is still applicable be-\ncause it only needs to modify the item value. In this case, we\ndo not have the security-privacy trade-off because the overall\ngain of RIA does not rely on the privacy budget.\nRIA without perturbation: A variant of RIA is that a fake\nuser samples one of the r target items randomly, encodes it,\nand sends the encoded value to the central server without\nperturbing it. When\nr = 1, this RIA variant has the same\noverall gain as MGA. Whenr > 1, the RIA variant uses a fake\nuser to promote only one target item. However, MGA uses\na fake user to simultaneously promote multiple target items,\nwhich means that its overall gain is multiple times of the RIA\nvariant’s overall gain. Moreover, it may be easy for the central\nserver to detect the RIA variant for OUE. Speciﬁcally, the\nserver can count the number of 1’s in a vector from a user. If\nthere is only one entry that is 1, then it is likely that the vector\nis from a fake user as the probability that a genuine vector\ncontains a single 1 is fairly small.\nDefending OLH by restricting the hash functions:Since\nMGA to OLH relies on searching a hash function that maps\ntarget items to the same hash value, the server could restrict\nthe space of seeds of the hash function or select the hash\nfunction by itself to defend OLH against MGA. However, the\ndefense may break the privacy guarantees. In particular, an\nuntrusted server could carefully select a space of seeds or a\nhash function that does not have collisions in the item domain.\nFor instance, a hash value h corresponds to a unique item.\nWhen receiving a hash value h from a user, the server knows\nthe user’s item, which breaks the LDP guarantee.\n8 Conclusion\nIn this work, we perform the ﬁrst systematic study on data\npoisoning attacks to LDP protocols. Our results show that\nan attacker can inject fake users to an LDP protocol and\nsend carefully crafted data to the server such that the target\nitems are estimated to have high frequencies or promoted as\nheavy hitters. We show that we can formulate such an attack\nas an optimization problem, solving which an attacker can\nmaximize its attack effectiveness. We theoretically and/or\nempirically show the effectiveness of our attacks. Moreover,\nwe explore three countermeasures against our attacks. Our\nempirical results show that these countermeasures have lim-\nited effectiveness in some scenarios, highlighting the needs\nfor new defenses against our attacks.\nInteresting future work includes generalizing our attacks\nto other LDP protocols, e.g., LDP protocols for itemset min-\ning [61] and key-value pairs [66], as well as developing new\ndefenses to mitigate our attacks.\nAcknowledgements\nWe thank the anonymous reviewers for their constructive com-\nments. The conditional probability based detection method\nfor one target item was suggested by a reviewer. This work\nwas supported by NSF grant No.1937786.\nReferences\n[1] Equifax Announces Cybersecurity Incident Involving\nConsumer Information. http://bit.ly/2PEHuPk, 2017.\n[2] A hacker gained access to 100 million Capital One credit\ncard applications and accounts. https://cnn.it/2WINTKV,\n2019.\n[3] In systemic breach, hackers steal millions of Bulgarians’\nﬁnancial data. https://reut.rs/2r6sMq3, 2019.\n[4] San francisco ﬁre department calls for service.\nhttp://bit.ly/336sddL, 2019.\n[5] Milton Abramowitz and Irene A Stegun. Handbook\nof mathematical functions: with formulas, graphs, and\nmathematical tables. Courier Corporation, 1965.\n[6] Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami.\nMining association rules between sets of items in large\ndatabases. In SIGMOD, 1993.\n\n[7] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poi-\nsoning attacks against autoregressive models. In AAAI,\n2016.\n[8] Brendan Avent, Aleksandra Korolova, David Zeber,\nTorgeir Hovden, and Benjamin Livshits. BLENDER:\nEnabling local search with a hybrid differential privacy\nmodel. In USENIX Security, 2017.\n[9] Raef Bassily, Kobbi Nissim, Uri Stemmer, and\nAbhradeep Guha Thakurta. Practical locally private\nheavy hitters. In NeurIPS, 2017.\n[10] Raef Bassily and Adam Smith. Local, private, efﬁcient\nprotocols for succinct histograms. In STOC, 2015.\n[11] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poi-\nsoning attacks against support vector machines. In\nICML, 2012.\n[12] Qiang Cao, Xiaowei Yang, Jieqi Yu, and Christopher\nPalow. Uncovering large groups of active malicious\naccounts in online social networks. In CCS, 2014.\n[13] Albert Cheu, Adam Smith, and Jonathan Ullman. Ma-\nnipulation attacks in local differential privacy. arXiv,\n2019.\n[14] Yann Collet. xxhash: Extremely fast hash algorithm.\nhttps://github.com/Cyan4973/xxHash, 2016.\n[15] Graham Cormode, Tejas Kulkarni, and Divesh Srivas-\ntava. Marginal release under local differential privacy.\nIn SIGMOD, 2018.\n[16] George Danezis and Prateek Mittal. Sybilinfer: Detect-\ning sybil nodes using social networks. In NDSS, 2009.\n[17] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin.\nCollecting telemetry data privately. In NeurIPS, 2017.\n[18] John C Duchi, Michael I Jordan, and Martin J Wain-\nwright. Local privacy and statistical minimax rates. In\nFOCS, 2013.\n[19] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. Calibrating noise to sensitivity in private\ndata analysis. In TCC, 2006.\n[20] Cynthia Dwork and Moni Naor. Pricing via processing\nor combatting junk mail. In CRYPTO, 1992.\n[21] Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth\nRaghunathan, Kunal Talwar, and Abhradeep Thakurta.\nAmpliﬁcation by shufﬂing: From local to central differ-\nential privacy via anonymity. In SODA, 2019.\n[22] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova.\nRappor: Randomized aggregatable privacy-preserving\nordinal response. In CCS, 2014.\n[23] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil\nGong. Local model poisoning attacks to byzantine-\nrobust federated learning. In USENIX Security, 2020.\n[24] Minghong Fang, Neil Zhenqiang Gong, and Jia Liu. In-\nﬂuence function based data poisoning attacks to top-n\nrecommender systems. In WWW, 2020.\n[25] Minghong Fang, Guolei Yang, Neil Zhenqiang Gong,\nand Jia Liu. Poisoning attacks to graph-based recom-\nmender systems. In ACSAC, 2018.\n[26] Neil Zhenqiang Gong, Mario Frank, and Prateek Mittal.\nSybilbelief: A semi-supervised learning approach for\nstructure-based sybil detection. TIFS, 2014.\n[27] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Sid-\ndharth Garg. Badnets: Evaluating backdooring attacks\non deep neural networks. IEEE Access, 2019.\n[28] Ling Huang, Anthony D Joseph, Blaine Nelson, Ben-\njamin IP Rubinstein, and J Doug Tygar. Adversarial\nmachine learning. In AISec, 2011.\n[29] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang\nLiu, Cristina Nita-Rotaru, and Bo Li. Manipulating ma-\nchine learning: Poisoning attacks and countermeasures\nfor regression learning. In S&P, 2018.\n[30] Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong.\nIntrinsic certiﬁed robustness of bagging against data\npoisoning attacks. AAAI, 2021.\n[31] Jinyuan Jia and Neil Zhenqiang Gong. Calibrate: Fre-\nquency estimation and heavy hitter identiﬁcation with\nlocal differential privacy via incorporating prior knowl-\nedge. In INFOCOM, 2019.\n[32] Peter Kairouz, Keith Bonawitz, and Daniel Ramage. Dis-\ncrete distribution estimation under local privacy. In\nICML, 2016.\n[33] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.\nExtremal mechanisms for local differential privacy. In\nNeurIPS, 2014.\n[34] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.\nSecure multi-party differential privacy. In NeurIPS,\n2015.\n[35] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy V orob-\neychik. Data poisoning attacks on factorization-based\ncollaborative ﬁltering. In NeurIPS, 2016.\n[36] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee,\nJuan Zhai, Weihang Wang, and Xiangyu Zhang. Trojan-\ning attack on neural networks. In NDSS, 2018.\n\n[37] Shike Mei and Xiaojin Zhu. Using machine teaching to\nidentify optimal training-set attacks on machine learners.\nIn AAAI, 2015.\n[38] Mehran Mozaffari-Kermani, Susmita Sur-Kolay, Anand\nRaghunathan, and Niraj K Jha. Systematic poisoning\nattacks on and defenses for machine learning in health-\ncare. IEEE journal of biomedical and health informatics,\n2014.\n[39] Luis Muñoz-González, Battista Biggio, Ambra Demon-\ntis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu,\nand Fabio Roli. Towards poisoning of deep learning al-\ngorithms with back-gradient optimization. In AISec,\n2017.\n[40] Moni Naor, Benny Pinkas, and Eyal Ronen. How to\n(not) share a password: Privacy preserving protocols for\nﬁnding heavy hitters with adversarial behavior. In CCS,\n2019.\n[41] Blaine Nelson, Marco Barreno, Fuching Jack Chi, An-\nthony D Joseph, Benjamin IP Rubinstein, Udam Saini,\nCharles A Sutton, J Doug Tygar, and Kai Xia. Exploit-\ning machine learning to subvert your spam ﬁlter. LEET,\n2008.\n[42] Andrew Newell, Rahul Potharaju, Luojie Xiang, and\nCristina Nita-Rotaru. On the practicality of integrity\nattacks on document-level sentiment analysis. In AISec,\n2014.\n[43] James Newsome, Brad Karp, and Dawn Song. Para-\ngraph: Thwarting signature learning by training mali-\nciously. In RAID workshop, 2006.\n[44] Roberto Perdisci, David Dagon, Wenke Lee, Prahlad\nFogla, and Monirul Sharif. Misleading worm signature\ngenerators using deliberate noise injection. In S&P,\n2006.\n[45] Zhan Qin, Yin Yang, Ting Yu, Issa Khalil, Xiaokui Xiao,\nand Kui Ren. Heavy hitter estimation over set-valued\ndata with local differential privacy. In CCS, 2016.\n[46] Sebastian Raschka. Mlxtend: Providing machine learn-\ning and data science utilities and extensions to python’s\nscientiﬁc computing stack. The Journal of Open Source\nSoftware, 2018.\n[47] Xuebin Ren, Chia-Mu Yu, Weiren Yu, Shusen Yang,\nXinyu Yang, Julie A McCann, and S Yu Philip. LoPub:\nHigh-dimensional crowdsourced data publication with\nlocal differential privacy. TIFS, 2018.\n[48] Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ash-\nwin Machanavajjhala, and Somesh Jha. Cryptε: Crypto-\nassisted differential privacy on untrusted servers. In\nSIGMOD, 2020.\n[49] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang,\nAnthony D Joseph, Shing-hon Lau, Satish Rao, Nina\nTaft, and J Doug Tygar. Antidote: understanding and\ndefending against poisoning of anomaly detectors. In\nIMC, 2009.\n[50] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian\nSuciu, Christoph Studer, Tudor Dumitras, and Tom Gold-\nstein. Poison frogs! targeted clean-label poisoning at-\ntacks on neural networks. In NeurIPS, 2018.\n[51] Ruggles Steven, Flood Sarah, Goeken Ronald, Grover\nJosiah, Meyer Erin, Pacas Jose, and Sobek Matthew.\nIpums usa: Version 9.0 [dataset]. minneapolis, mn:\nIpums, 2019. https://doi.org/10.18128/D010.V9.0,\n2019.\n[52] Gianluca Stringhini, Christopher Kruegel, and Giovanni\nVigna. Detecting spammers on social networks. In\nACSAC, 2010.\n[53] Apple Differential Privacy Team. Learning with privacy\nat scale. Machine Learning Journal, 2017.\n[54] Kurt Thomas, Damon McCoy, Chris Grier, Alek Kolcz,\nand Vern Paxson. Trafﬁcking fraudulent accounts: The\nrole of the underground market in twitter spam and\nabuse. In USENIX Security, 2013.\n[55] Binghui Wang and Neil Zhenqiang Gong. Attacking\ngraph-based classiﬁcation via manipulating the graph\nstructure. In CCS, 2019.\n[56] Binghui Wang, Jinyuan Jia, and Neil Zhenqiang Gong.\nGraph-based security and privacy analytics via collec-\ntive classiﬁcation with joint weight learning and propa-\ngation. In NDSS, 2019.\n[57] Gang Wang, Tristan Konolige, Christo Wilson, Xiao\nWang, Haitao Zheng, and Ben Y Zhao. You are how\nyou click: Clickstream analysis for sybil detection. In\nUSENIX Security, 2013.\n[58] Gang Wang, Tianyi Wang, Haitao Zheng, and Ben Y\nZhao. Man vs. machine: Practical adversarial detec-\ntion of malicious crowdsourcing workers. In USENIX\nSecurity, 2014.\n[59] Tianhao Wang, Jeremiah Blocki, Ninghui Li, and\nSomesh Jha. Locally differentially private protocols\nfor frequency estimation. In USENIX Security, 2017.\n[60] Tianhao Wang, Bolin Ding, Jingren Zhou, Cheng Hong,\nZhicong Huang, Ninghui Li, and Somesh Jha. Answer-\ning multi-dimensional analytical queries under local dif-\nferential privacy. In SIGMOD, 2019.\n\n[61] Tianhao Wang, Ninghui Li, and Somesh Jha. Locally\ndifferentially private frequent itemset mining. In S&P,\n2018.\n[62] Tianhao Wang, Ninghui Li, and Somesh Jha. Locally\ndifferentially private heavy hitter identiﬁcation. TDSC,\n2019.\n[63] Tianhao Wang, Milan Lopuhaä-Zwakenberg, Zitao Li,\nBoris Skoric, and Ninghui Li. Locally differentially\nprivate frequency estimation with consistency. In NDSS,\n2020.\n[64] Stanley L Warner. Randomized response: A survey\ntechnique for eliminating evasive answer bias. Journal\nof the American Statistical Association, 1965.\n[65] Guolei Yang, Neil Zhenqiang Gong, and Ying Cai. Fake\nco-visitation injection attacks to recommender systems.\nIn NDSS, 2017.\n[66] Qingqing Ye, Haibo Hu, Xiaofeng Meng, and Huadi\nZheng. Privkv: Key-value data collection with local\ndifferential privacy. In S&P, 2019.\n[67] Haifeng Yu, Haifeng Yu, Michael Kaminsky, Phillip B\nGibbons, and Abraham Flaxman. Sybilguard: defending\nagainst sybil attacks via social networks. In SIGCOMM,\n2006.\n[68] Dong Yuan, Yuanli Miao, Neil Zhenqiang Gong, Zheng\nYang, Qi Li, Dawn Song, Qian Wang, and Xiao Liang.\nDetecting fake accounts in online social networks at the\ntime of registrations. In CCS, 2019.\n[69] Zhikun Zhang, Tianhao Wang, Ninghui Li, Shibo He,\nand Jiming Chen. Calm: Consistent adaptive local\nmarginal for marginal release under local differential\nprivacy. In CCS, 2018.\nA Proof of Theorem 2\nProof. Let β(1 − fT )+ β(d−r)\neε−1 > β(2r − fT )+ 2βr\neε−1 ,w eh a v e :\n1 + d −r\neε −1 > 2r + 2r\neε −1 ⇐⇒ d −3r\neε −1 > 2r −1. (42)\nSince eε > 1, the inequality above is equivalent to d > (2r −\n1)(eε −1)+ 3r.\nB FPRs of Detecting Fake Users\nOUE: If a user’s perturbed binary vector yyy follows the\nOUE protocol, then we can calculate the probability that the\nitems in a set B of size z, are all 1 in the perturbed binary\nvector as follows: Pr(yb = 1,∀b ∈ B)= pqz−1 if v ∈ B and\nPr(yb = 1,∀b ∈B)= qz otherwise, where yb is the bth bit of\nthe perturbed binary vector yyy and v is the user’s item. Let\nfB = ∑b∈B fb denote the sum of true frequencies of all items\nin B, X1 denote the random variable representing the number\nof users whose items are in B and whose perturbed binary\nvectors are 1 for all items inB, and X2 denote the random vari-\nable representing the number of users whose items are not in\nB and whose perturbed binary vectors are 1 for all items in B.\nIf all the n + m users follow the OUE protocol, then we have\nthe following distributions: X1 ∼ Binom( fB(n + m), pqz−1)\nand X2 ∼Binom((1− fB)(n +m),qz), where Binom is a bino-\nmial distribution. Now we consider another random variable\nX = X1 + X2, which represents the number of users whose\nperturbed binary vectors are 1 for all items in B. X follows a\ndistribution with mean μ and variance Var as follows:\nμ = fB(n + m)pqz−1 +( 1 − fB)(n + m)qz (43)\n≤(n + m)pqz−1 (44)\nVar = fB(n + m)pqz−1(1 −pqz−1)\n+( 1 − fB)(n + m)qz(1 −qz) (45)\n≤(n + m)pqz−1(1 −pqz−1). (46)\nBased on the Chebyshev’s inequality, for any τz > (n +\nm)pqz−1,w eh a v e :\nPr(X ≥τz)= Pr(X −μ ≥τz −μ)\n≤Pr(|X −μ|≥ τz −μ)\n≤ Var\n(τz −μ)2\n≤ (n + m)pqz−1(1 −pqz−1)\n[τz −(n + m)pqz−1]2 (47)\nHere, if we choose τz as the threshold, the probability Pr(X ≥\nτz) is the false positive rate, which is upper bounded by\n(n+m)pqz−1(1−pqz−1)\n[τz−(n+m)pqz−1]2 .\nOLH: As discussed in Section 6.2, we ﬁrst construct a d-bit\nbinary vector yyy for each user with a tuple (H,a) such that\nyv = 1 if and only if H(v)= a. For an item set B of size z,\nassume X is a random variable that represents the number of\nusers whose constructed binary vectors are 1’s for all items\nin B. If all the n + m users follow the OLH protocol, then for\nany τz > 0, the probability that X ≥τz is bounded as follows:\nPr(X ≥τz)= 1 −Pr(X ≤τz −1)\n= 1 −I(1 −qz−1;n + m −τz + 1,τz)\n= I(qz−1;τz,n + m −τz + 1) (48)\nNote that if we set τz as the threshold, the probability Pr(X ≥\nτz) is the false positive rate."
}